{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7174d8b4",
   "metadata": {},
   "source": [
    "# 02 - Feature Engineering & Vectorization\n",
    "\n",
    "This notebook loads the curated feature table, engineers additional signals, prepares consistent train/test splits, and materializes the preprocessing pipeline used by downstream models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8e5d8",
   "metadata": {},
   "source": [
    "## How to use\n",
    "1. Ensure `01_data_ingestion.ipynb` has been executed successfully.\n",
    "2. Adjust the configuration cell if artifact paths differ.\n",
    "3. Run the notebook to create train/test splits and persist `preprocessor.joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8014559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install dependencies required for this notebook.\n",
    "# !pip install scikit-learn scipy joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c1901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8049b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ARTIFACT_DIR = NOTEBOOK_DIR / \"artifacts\"\n",
    "ARTIFACT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "METADATA_FEATURES_PATH = ARTIFACT_DIR / \"metadata_features.csv\"\n",
    "TRAIN_DATA_PATH = ARTIFACT_DIR / \"train_dataset.csv\"\n",
    "TEST_DATA_PATH = ARTIFACT_DIR / \"test_dataset.csv\"\n",
    "PREPROCESSOR_PATH = ARTIFACT_DIR / \"preprocessor.joblib\"\n",
    "\n",
    "TARGET_COLUMN = \"business_capability\"\n",
    "\n",
    "TEXT_FEATURES = [\n",
    "    \"original_path_keywords\",\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    \"extension\",\n",
    "    \"extension_family\",\n",
    "]\n",
    "\n",
    "NUMERIC_FEATURES = [\n",
    "    \"original_path_depth\",\n",
    "    \"file_size_bytes\",\n",
    "    \"content_word_count\",\n",
    "]\n",
    "\n",
    "BINARY_FEATURES: list[str] = []\n",
    "\n",
    "FEATURE_COLUMNS = TEXT_FEATURES + CATEGORICAL_FEATURES + NUMERIC_FEATURES + BINARY_FEATURES\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "if not METADATA_FEATURES_PATH.exists():\n",
    "    raise FileNotFoundError(\"metadata_features.csv not found. Run 01_data_ingestion.ipynb first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf2b2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIRED_COLUMNS = FEATURE_COLUMNS + [TARGET_COLUMN]\n",
    "ENCODING_CANDIDATES = (\"utf-8\", \"utf-8-sig\", \"latin-1\", \"cp1252\")\n",
    "\n",
    "\n",
    "def load_metadata_features(path: Path) -> pd.DataFrame:\n",
    "    last_error = None\n",
    "    for encoding in ENCODING_CANDIDATES:\n",
    "        try:\n",
    "            df = pd.read_csv(path, encoding=encoding)\n",
    "            if encoding != \"utf-8\":\n",
    "                print(f\"Loaded {path.name} using fallback encoding '{encoding}'.\")\n",
    "            return df\n",
    "        except UnicodeDecodeError as unicode_error:\n",
    "            last_error = unicode_error\n",
    "        except pd.errors.ParserError as parser_error:\n",
    "            last_error = parser_error\n",
    "            try:\n",
    "                df = pd.read_csv(\n",
    "                    path,\n",
    "                    encoding=encoding,\n",
    "                    engine=\"python\",\n",
    "                    on_bad_lines=\"skip\",\n",
    "                )\n",
    "                print(\n",
    "                    f\"Loaded {path.name} with python engine using encoding '{encoding}' after handling malformed lines.\"\n",
    "                )\n",
    "                return df\n",
    "            except Exception as inner_error:\n",
    "                last_error = inner_error\n",
    "    message = f\"Failed to decode or parse {path} with encodings {ENCODING_CANDIDATES}.\"\n",
    "    if last_error is not None:\n",
    "        raise RuntimeError(message) from last_error\n",
    "    raise RuntimeError(message)\n",
    "\n",
    "\n",
    "df = load_metadata_features(METADATA_FEATURES_PATH)\n",
    "\n",
    "missing_columns = [col for col in REQUIRED_COLUMNS if col not in df.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns in metadata dataset: {missing_columns}\")\n",
    "\n",
    "for column in TEXT_FEATURES + [\"business_capability\"]:\n",
    "    if column in df:\n",
    "        df[column] = df[column].fillna(\"\").astype(str)\n",
    "\n",
    "for column in CATEGORICAL_FEATURES:\n",
    "    if column in df:\n",
    "        df[column] = df[column].fillna(\"\").astype(str)\n",
    "\n",
    "for column in NUMERIC_FEATURES:\n",
    "    if column in df:\n",
    "        df[column] = pd.to_numeric(df[column], errors=\"coerce\").fillna(0)\n",
    "\n",
    "for column in BINARY_FEATURES:\n",
    "    if column in df:\n",
    "        df[column] = pd.to_numeric(df[column], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "text_non_empty_counts = {}\n",
    "ACTIVE_TEXT_FEATURES = []\n",
    "for column in TEXT_FEATURES:\n",
    "    if column in df:\n",
    "        non_empty = int((df[column].astype(str).str.strip() != \"\").sum())\n",
    "        text_non_empty_counts[column] = non_empty\n",
    "        if non_empty > 0:\n",
    "            ACTIVE_TEXT_FEATURES.append(column)\n",
    "\n",
    "print(\"Text feature non-empty counts:\", text_non_empty_counts)\n",
    "\n",
    "inactive_text_features = [col for col, count in text_non_empty_counts.items() if count == 0]\n",
    "if inactive_text_features:\n",
    "    print(\"Skipping empty text features:\", inactive_text_features)\n",
    "else:\n",
    "    print(\"All text features contain content.\")\n",
    "\n",
    "FEATURE_COLUMNS = ACTIVE_TEXT_FEATURES + CATEGORICAL_FEATURES + NUMERIC_FEATURES + BINARY_FEATURES\n",
    "\n",
    "if not ACTIVE_TEXT_FEATURES:\n",
    "    print(\"Warning: no text features contain content; TF-IDF vectorizers will be skipped.\")\n",
    "\n",
    "df[FEATURE_COLUMNS].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e316192",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=df[TARGET_COLUMN],\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "train_df.to_csv(TRAIN_DATA_PATH, index=False)\n",
    "test_df.to_csv(TEST_DATA_PATH, index=False)\n",
    "\n",
    "print(f\"Train rows: {len(train_df)}\")\n",
    "print(f\"Test rows: {len(test_df)}\")\n",
    "print(\"Class distribution (train):\")\n",
    "print(train_df[TARGET_COLUMN].value_counts(normalize=True))\n",
    "print(\"Class distribution (test):\")\n",
    "print(test_df[TARGET_COLUMN].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692b51ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_vectorizer = TfidfVectorizer(\n",
    "    max_features=35000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=1,\n",
    "    strip_accents=\"unicode\",\n",
    ")\n",
    "\n",
    "path_vectorizer = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=1,\n",
    "    strip_accents=\"unicode\",\n",
    ")\n",
    "\n",
    "original_path_vectorizer = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=1,\n",
    "    strip_accents=\"unicode\",\n",
    ")\n",
    "\n",
    "file_name_vectorizer = TfidfVectorizer(\n",
    "    max_features=15000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=1,\n",
    "    strip_accents=\"unicode\",\n",
    ")\n",
    "\n",
    "notes_vectorizer = TfidfVectorizer(\n",
    "    max_features=15000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=1,\n",
    "    strip_accents=\"unicode\",\n",
    ")\n",
    "\n",
    "transformers = []\n",
    "\n",
    "if \"content_text\" in ACTIVE_TEXT_FEATURES:\n",
    "    transformers.append((\"content_tfidf\", content_vectorizer, \"content_text\"))\n",
    "if \"path_keywords\" in ACTIVE_TEXT_FEATURES:\n",
    "    transformers.append((\"path_tfidf\", path_vectorizer, \"path_keywords\"))\n",
    "if \"original_path_keywords\" in ACTIVE_TEXT_FEATURES:\n",
    "    transformers.append((\"original_path_tfidf\", original_path_vectorizer, \"original_path_keywords\"))\n",
    "if \"file_name_keywords\" in ACTIVE_TEXT_FEATURES:\n",
    "    transformers.append((\"file_name_tfidf\", file_name_vectorizer, \"file_name_keywords\"))\n",
    "if \"notes_text\" in ACTIVE_TEXT_FEATURES:\n",
    "    transformers.append((\"notes_tfidf\", notes_vectorizer, \"notes_text\"))\n",
    "\n",
    "if CATEGORICAL_FEATURES:\n",
    "    transformers.append((\"categorical\", OneHotEncoder(handle_unknown=\"ignore\"), CATEGORICAL_FEATURES))\n",
    "\n",
    "numeric_like_features = NUMERIC_FEATURES + BINARY_FEATURES\n",
    "if numeric_like_features:\n",
    "    transformers.append((\"numeric\", StandardScaler(with_mean=False), numeric_like_features))\n",
    "\n",
    "if not transformers:\n",
    "    raise ValueError(\"No transformers configured. Verify feature availability.\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=transformers,\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9ef865",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(preprocessor, PREPROCESSOR_PATH)\n",
    "print(f\"Saved preprocessing configuration to {PREPROCESSOR_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bacc231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "preprocessor_check = clone(preprocessor)\n",
    "feature_matrix_sample = preprocessor_check.fit_transform(train_df[FEATURE_COLUMNS], train_df[TARGET_COLUMN])\n",
    "print(\"Sample feature matrix shape (train split):\", feature_matrix_sample.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
