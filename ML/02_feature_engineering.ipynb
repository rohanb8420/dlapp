{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7174d8b4",
   "metadata": {},
   "source": [
    "# 02 - Feature Engineering & Vectorization\n",
    "\n",
    "This notebook loads the curated feature table, engineers additional signals, prepares consistent train/test splits, and materializes the preprocessing pipeline used by downstream models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8e5d8",
   "metadata": {},
   "source": [
    "## How to use\n",
    "1. Ensure `01_data_ingestion.ipynb` has been executed successfully.\n",
    "2. Adjust the configuration cell if artifact paths differ.\n",
    "3. Run the notebook to create train/test splits and persist `preprocessor.joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8014559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install dependencies required for this notebook.\n",
    "# !pip install scikit-learn scipy joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c1901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8049b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ARTIFACT_DIR = NOTEBOOK_DIR / \"artifacts\"\n",
    "ARTIFACT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "METADATA_FEATURES_PATH = ARTIFACT_DIR / \"metadata_features.csv\"\n",
    "TRAIN_DATA_PATH = ARTIFACT_DIR / \"train_dataset.csv\"\n",
    "TEST_DATA_PATH = ARTIFACT_DIR / \"test_dataset.csv\"\n",
    "PREPROCESSOR_PATH = ARTIFACT_DIR / \"preprocessor.joblib\"\n",
    "\n",
    "TARGET_COLUMN = \"business_capability\"\n",
    "\n",
    "TEXT_FEATURES = [\n",
    "    \"content_text\",\n",
    "    \"path_keywords\",\n",
    "    \"original_path_keywords\",\n",
    "    \"file_name_keywords\",\n",
    "    \"notes_text\",\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    \"extension\",\n",
    "    \"extension_family\",\n",
    "    \"record_type\",\n",
    "    \"retention_code\",\n",
    "]\n",
    "\n",
    "NUMERIC_FEATURES = [\n",
    "    \"path_depth\",\n",
    "    \"original_path_depth\",\n",
    "    \"path_token_count\",\n",
    "    \"original_path_token_count\",\n",
    "    \"path_char_len\",\n",
    "    \"original_path_char_len\",\n",
    "    \"file_name_char_len\",\n",
    "    \"file_name_digit_count\",\n",
    "    \"file_size_bytes\",\n",
    "    \"modified_time_epoch\",\n",
    "    \"content_char_len\",\n",
    "    \"content_word_count\",\n",
    "    \"notes_word_count\",\n",
    "]\n",
    "\n",
    "BINARY_FEATURES = [\n",
    "    \"file_exists\",\n",
    "    \"has_content\",\n",
    "]\n",
    "\n",
    "FEATURE_COLUMNS = TEXT_FEATURES + CATEGORICAL_FEATURES + NUMERIC_FEATURES + BINARY_FEATURES\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "if not METADATA_FEATURES_PATH.exists():\n",
    "    raise FileNotFoundError(\"metadata_features.csv not found. Run 01_data_ingestion.ipynb first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf2b2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(METADATA_FEATURES_PATH)\n",
    "\n",
    "for column in TEXT_FEATURES + [\"business_capability\"]:\n",
    "    if column in df:\n",
    "        df[column] = df[column].fillna(\"\").astype(str)\n",
    "\n",
    "for column in CATEGORICAL_FEATURES:\n",
    "    if column in df:\n",
    "        df[column] = df[column].fillna(\"\").astype(str)\n",
    "\n",
    "for column in NUMERIC_FEATURES:\n",
    "    if column in df:\n",
    "        df[column] = pd.to_numeric(df[column], errors=\"coerce\").fillna(0)\n",
    "\n",
    "for column in BINARY_FEATURES:\n",
    "    if column in df:\n",
    "        df[column] = pd.to_numeric(df[column], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "df[FEATURE_COLUMNS].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e316192",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=df[TARGET_COLUMN],\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "train_df.to_csv(TRAIN_DATA_PATH, index=False)\n",
    "test_df.to_csv(TEST_DATA_PATH, index=False)\n",
    "\n",
    "print(f\"Train rows: {len(train_df)}\")\n",
    "print(f\"Test rows: {len(test_df)}\")\n",
    "print(\"Class distribution (train):\")\n",
    "print(train_df[TARGET_COLUMN].value_counts(normalize=True))\n",
    "print(\"Class distribution (test):\")\n",
    "print(test_df[TARGET_COLUMN].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692b51ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_vectorizer = TfidfVectorizer(\n",
    "    max_features=35000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    strip_accents=\"unicode\",\n",
    ")\n",
    "\n",
    "path_vectorizer = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=1,\n",
    "    strip_accents=\"unicode\",\n",
    ")\n",
    "\n",
    "original_path_vectorizer = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=1,\n",
    "    strip_accents=\"unicode\",\n",
    ")\n",
    "\n",
    "file_name_vectorizer = TfidfVectorizer(\n",
    "    max_features=15000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=1,\n",
    "    strip_accents=\"unicode\",\n",
    ")\n",
    "\n",
    "notes_vectorizer = TfidfVectorizer(\n",
    "    max_features=15000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=1,\n",
    "    strip_accents=\"unicode\",\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"content_tfidf\", content_vectorizer, \"content_text\"),\n",
    "        (\"path_tfidf\", path_vectorizer, \"path_keywords\"),\n",
    "        (\"original_path_tfidf\", original_path_vectorizer, \"original_path_keywords\"),\n",
    "        (\"file_name_tfidf\", file_name_vectorizer, \"file_name_keywords\"),\n",
    "        (\"notes_tfidf\", notes_vectorizer, \"notes_text\"),\n",
    "        (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\"), CATEGORICAL_FEATURES),\n",
    "        (\"numeric\", StandardScaler(with_mean=False), NUMERIC_FEATURES + BINARY_FEATURES),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9ef865",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(preprocessor, PREPROCESSOR_PATH)\n",
    "print(f\"Saved preprocessing configuration to {PREPROCESSOR_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bacc231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "preprocessor_check = clone(preprocessor)\n",
    "feature_matrix_sample = preprocessor_check.fit_transform(train_df[FEATURE_COLUMNS], train_df[TARGET_COLUMN])\n",
    "print(\"Sample feature matrix shape (train split):\", feature_matrix_sample.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}