{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aeed504",
   "metadata": {},
   "source": [
    "# 02 - Feature Engineering & Vectorization\n",
    "\n",
    "This notebook loads the curated feature table, engineers additional signals, prepares consistent train/test splits, and materializes the preprocessing pipeline used by downstream models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e958a",
   "metadata": {},
   "source": [
    "## How to use\n",
    "1. Ensure `01_data_ingestion.ipynb` has been executed successfully.\n",
    "2. Update the configuration cell if the artifact paths differ.\n",
    "3. Run the notebook to create train/test splits and persist `preprocessor.joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9446ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install dependencies required for vectorization.\n",
    "# !pip install scikit-learn scipy joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3635d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ARTIFACT_DIR = NOTEBOOK_DIR / 'artifacts'\n",
    "ARTIFACT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "METADATA_FEATURES_PATH = ARTIFACT_DIR / 'metadata_features.csv'\n",
    "TRAIN_DATA_PATH = ARTIFACT_DIR / 'train_dataset.csv'\n",
    "TEST_DATA_PATH = ARTIFACT_DIR / 'test_dataset.csv'\n",
    "PREPROCESSOR_PATH = ARTIFACT_DIR / 'preprocessor.joblib'\n",
    "\n",
    "TARGET_COLUMN = 'business_capability'\n",
    "FEATURE_COLUMNS = [\n",
    "    'content_text',\n",
    "    'path_keywords',\n",
    "    'extension',\n",
    "    'path_depth',\n",
    "    'original_path_depth',\n",
    "    'path_token_count',\n",
    "    'content_char_len',\n",
    "    'content_word_count',\n",
    "    'file_exists',\n",
    "]\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "if not METADATA_FEATURES_PATH.exists():\n",
    "    raise FileNotFoundError('metadata_features.csv not found. Run 01_data_ingestion.ipynb first.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a4687",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(METADATA_FEATURES_PATH)\n",
    "\n",
    "for column in ['content_text', 'path_keywords', 'extension']:\n",
    "    if column in df:\n",
    "        df[column] = df[column].fillna('').astype(str)\n",
    "\n",
    "numeric_columns = ['path_depth', 'original_path_depth', 'path_token_count', 'content_char_len', 'content_word_count', 'file_exists']\n",
    "for column in numeric_columns:\n",
    "    if column in df:\n",
    "        df[column] = df[column].fillna(0)\n",
    "\n",
    "if TARGET_COLUMN not in df.columns:\n",
    "    raise ValueError(f'Missing target column `{TARGET_COLUMN}` in metadata dataset.')\n",
    "\n",
    "df[FEATURE_COLUMNS].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd255e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=df[TARGET_COLUMN],\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "train_df.to_csv(TRAIN_DATA_PATH, index=False)\n",
    "test_df.to_csv(TEST_DATA_PATH, index=False)\n",
    "\n",
    "print(f'Train rows: {len(train_df)}')\n",
    "print(f'Test rows: {len(test_df)}')\n",
    "print('Class distribution (train):')\n",
    "print(train_df[TARGET_COLUMN].value_counts(normalize=True))\n",
    "print('Class distribution (test):')\n",
    "print(test_df[TARGET_COLUMN].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59150b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer_content = TfidfVectorizer(\n",
    "    max_features=25000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    strip_accents='unicode',\n",
    ")\n",
    "\n",
    "text_vectorizer_path = TfidfVectorizer(\n",
    "    max_features=15000,\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=1,\n",
    "    strip_accents='unicode',\n",
    ")\n",
    "\n",
    "categorical_features = ['extension']\n",
    "numeric_features = ['path_depth', 'original_path_depth', 'path_token_count', 'content_char_len', 'content_word_count', 'file_exists']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('content_tfidf', text_vectorizer_content, 'content_text'),\n",
    "        ('path_tfidf', text_vectorizer_path, 'path_keywords'),\n",
    "        ('extension_ohe', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "        ('numeric', StandardScaler(with_mean=False), numeric_features),\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    sparse_threshold=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6ce6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(preprocessor, PREPROCESSOR_PATH)\n",
    "print(f'Saved preprocessing configuration to {PREPROCESSOR_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27119656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional sanity check: clone the preprocessor and fit on the training split to confirm it runs end-to-end.\n",
    "from sklearn.base import clone\n",
    "\n",
    "preprocessor_check = clone(preprocessor)\n",
    "feature_matrix_sample = preprocessor_check.fit_transform(train_df[FEATURE_COLUMNS], train_df[TARGET_COLUMN])\n",
    "print('Sample feature matrix shape (train split):', feature_matrix_sample.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}