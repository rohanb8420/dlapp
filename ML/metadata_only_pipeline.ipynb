{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02a8c5eb",
   "metadata": {},
   "source": [
    "# Metadata-Only Classification Pipeline\n",
    "\n",
    "End-to-end workflow that ingests the lifecycle catalog, derives metadata-based features (no file content reads), and trains LightGBM/XGBoost models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0875cb7",
   "metadata": {},
   "source": [
    "## Notebook Outline\n",
    "- Configure paths and dependencies\n",
    "- Ingest the catalog (Excel) and derive metadata features\n",
    "- Build the modeling dataset using path-based features only\n",
    "- Train/test split and preprocessing pipeline\n",
    "- Benchmark LightGBM and XGBoost classifiers\n",
    "- Persist artifacts under `artifacts_metadata_only/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73897b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install dependencies before first run.\n",
    "# !pip install pandas openpyxl scikit-learn lightgbm xgboost joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff813461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5be2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(transform_output='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d4b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "EXCEL_PATH = PROJECT_ROOT / 'assets' / 'training_data.xlsx'\n",
    "RAW_BASE_DIR = PROJECT_ROOT\n",
    "\n",
    "OUTPUT_DIR = Path.cwd() / 'artifacts_metadata_only'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CURATED_DATA_PATH = OUTPUT_DIR / 'metadata_features_metadata_only.csv'\n",
    "TRAIN_DATA_PATH = OUTPUT_DIR / 'train_dataset_metadata_only.csv'\n",
    "TEST_DATA_PATH = OUTPUT_DIR / 'test_dataset_metadata_only.csv'\n",
    "MODEL_DIR = OUTPUT_DIR / 'models'\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "MODEL_COMPARISON_PATH = OUTPUT_DIR / 'model_comparison.csv'\n",
    "\n",
    "TARGET_COLUMN = 'business_capability'\n",
    "TEXT_FEATURE = 'original_path_keywords'\n",
    "CATEGORICAL_FEATURES = ['extension', 'extension_family']\n",
    "NUMERIC_FEATURES = ['original_path_depth', 'file_size_bytes']\n",
    "FEATURE_COLUMNS = [TEXT_FEATURE] + CATEGORICAL_FEATURES + NUMERIC_FEATURES\n",
    "\n",
    "REQUIRED_COLUMNS = ['Original File Path', 'File Path', 'Business Capability']\n",
    "OPTIONAL_COLUMNS = ['Record Type', 'Retention Code', 'Notes']\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "CV_FOLDS = 3  # Reduced to handle rare classes\n",
    "\n",
    "print(f\"Excel catalog path: {EXCEL_PATH}\")\n",
    "print(f\"Artifacts directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51167bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "RENAME_MAP = {\n",
    "    'Original File Path': 'original_file_path',\n",
    "    'File Path': 'file_path',\n",
    "    'Business Capability': 'business_capability',\n",
    "    'Record Type': 'record_type',\n",
    "    'Retention Code': 'retention_code',\n",
    "    'Notes': 'notes_text',\n",
    "}\n",
    "\n",
    "\n",
    "def load_catalog(path: Path, required_columns: Iterable[str], optional_columns: Iterable[str]) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'Catalog not found: {path}')\n",
    "    df = pd.read_excel(path)\n",
    "    missing = [col for col in required_columns if col not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f'Missing required columns in catalog: {missing}')\n",
    "    available_optional = [col for col in optional_columns if col in df.columns]\n",
    "    df = df[list(required_columns) + available_optional].copy()\n",
    "    df = df.rename(columns=RENAME_MAP)\n",
    "    for column in ['original_file_path', 'file_path', 'business_capability']:\n",
    "        df[column] = df[column].astype(str).str.strip()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64176b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORD_CLEANER = re.compile(r'[^A-Za-z0-9]+')\n",
    "\n",
    "\n",
    "def safe_path(value: str | float | int | None) -> str:\n",
    "    if value is None:\n",
    "        return ''\n",
    "    if isinstance(value, float) and np.isnan(value):\n",
    "        return ''\n",
    "    return str(value).strip()\n",
    "\n",
    "\n",
    "def resolve_path(raw_value: str, base_dir: Path) -> Path:\n",
    "    candidate_str = safe_path(raw_value)\n",
    "    if not candidate_str:\n",
    "        return base_dir\n",
    "    try:\n",
    "        candidate = Path(candidate_str)\n",
    "    except Exception:\n",
    "        candidate = base_dir / candidate_str\n",
    "    if not candidate.is_absolute():\n",
    "        candidate = base_dir / candidate\n",
    "    try:\n",
    "        return candidate.resolve(strict=False)\n",
    "    except Exception:\n",
    "        return candidate\n",
    "\n",
    "\n",
    "def to_keywords_from_path(path: Path) -> str:\n",
    "    tokens = []\n",
    "    for part in path.parts:\n",
    "        cleaned = KEYWORD_CLEANER.sub(' ', part).strip().lower()\n",
    "        if cleaned:\n",
    "            tokens.append(cleaned)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def path_depth(path: Path) -> int:\n",
    "    return len(path.parts)\n",
    "\n",
    "\n",
    "EXTENSION_FAMILY_MAP = {\n",
    "    '.xls': 'excel',\n",
    "    '.xlsx': 'excel',\n",
    "    '.xlsm': 'excel',\n",
    "    '.xlsb': 'excel',\n",
    "    '.csv': 'tabular',\n",
    "    '.tsv': 'tabular',\n",
    "    '.txt': 'text',\n",
    "    '.log': 'text',\n",
    "    '.json': 'json',\n",
    "    '.xml': 'markup',\n",
    "    '.yaml': 'markup',\n",
    "    '.yml': 'markup',\n",
    "    '.ini': 'config',\n",
    "    '.cfg': 'config',\n",
    "    '.conf': 'config',\n",
    "    '.doc': 'word',\n",
    "    '.docx': 'word',\n",
    "    '.pdf': 'pdf',\n",
    "    '.ppt': 'presentation',\n",
    "    '.pptx': 'presentation',\n",
    "    '.msg': 'outlook',\n",
    "    '.html': 'html',\n",
    "    '.htm': 'html',\n",
    "    '.css': 'code',\n",
    "    '.js': 'code',\n",
    "    '.sql': 'code',\n",
    "    '.py': 'code',\n",
    "    '.sas': 'sas',\n",
    "    '.sas7bdat': 'sas',\n",
    "    '.ipynb': 'notebook',\n",
    "    '.jpg': 'image',\n",
    "    '.jpeg': 'image',\n",
    "    '.png': 'image',\n",
    "    '.gif': 'image',\n",
    "    '.bmp': 'image',\n",
    "    '.vsd': 'visio',\n",
    "    '.vsdx': 'visio',\n",
    "    '.twb': 'tableau',\n",
    "    '.twbx': 'tableau',\n",
    "}\n",
    "\n",
    "\n",
    "def extension_family(suffix: str) -> str:\n",
    "    return EXTENSION_FAMILY_MAP.get(suffix.lower(), 'other')\n",
    "\n",
    "\n",
    "def gather_file_stats(path: Path) -> tuple[int, float]:\n",
    "    try:\n",
    "        stat_result = path.stat()\n",
    "        return stat_result.st_size, float(stat_result.st_mtime)\n",
    "    except OSError:\n",
    "        return 0, float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6013c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_illegal_characters(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove illegal or non-printable characters from all string columns in a DataFrame.\n",
    "    \"\"\"\n",
    "    illegal_chars = [\n",
    "        '\\x00', '\\x01', '\\x02', '\\x03', '\\x04', '\\x05',\n",
    "        '\\x06', '\\x07', '\\x08', '\\x0b', '\\x0c', '\\x0e',\n",
    "        '\\x0f', '\\x10', '\\x11', '\\x12', '\\x13', '\\x14',\n",
    "        '\\x15', '\\x16', '\\x17', '\\x18', '\\x19', '\\x1a'\n",
    "    ]\n",
    "\n",
    "    def clean_value(x):\n",
    "        if pd.isnull(x):\n",
    "            return x\n",
    "        return ''.join(c for c in str(x) if c.isprintable() and c not in illegal_chars)\n",
    "\n",
    "    str_cols = df.select_dtypes(include=['object']).columns\n",
    "    df[str_cols] = df[str_cols].applymap(clean_value)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f8f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_df = load_catalog(EXCEL_PATH, REQUIRED_COLUMNS, OPTIONAL_COLUMNS)\n",
    "\n",
    "catalog_df['resolved_path'] = catalog_df['file_path'].apply(lambda value: resolve_path(value, RAW_BASE_DIR))\n",
    "catalog_df['original_resolved_path'] = catalog_df['original_file_path'].apply(lambda value: resolve_path(value, RAW_BASE_DIR))\n",
    "catalog_df['extension'] = catalog_df['resolved_path'].apply(lambda p: p.suffix.lower())\n",
    "catalog_df['extension_family'] = catalog_df['extension'].apply(extension_family)\n",
    "catalog_df['original_path_depth'] = catalog_df['original_resolved_path'].apply(path_depth)\n",
    "catalog_df['original_path_keywords'] = catalog_df['original_resolved_path'].apply(to_keywords_from_path)\n",
    "\n",
    "size_mtime = catalog_df['resolved_path'].apply(gather_file_stats)\n",
    "catalog_df['file_size_bytes'] = [pair[0] for pair in size_mtime]\n",
    "\n",
    "catalog_df.head()\n",
    "\n",
    "catalog_df = remove_illegal_characters(catalog_df)\n",
    "cleaned_excel_path = OUTPUT_DIR / 'input_data_cleaned.xlsx'\n",
    "try:\n",
    "    catalog_df.to_excel(cleaned_excel_path, index=False, engine='openpyxl')\n",
    "    print(f'Saved cleaned catalog to {cleaned_excel_path}')\n",
    "except Exception as excel_error:\n",
    "    print('Excel export skipped:', excel_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5550244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = catalog_df[\n",
    "    [\n",
    "        'original_file_path',\n",
    "        'file_path',\n",
    "        'business_capability',\n",
    "        'original_path_keywords',\n",
    "        'extension',\n",
    "        'extension_family',\n",
    "        'original_path_depth',\n",
    "        'file_size_bytes',\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "feature_df['extension'] = feature_df['extension'].fillna('').astype(str)\n",
    "feature_df['extension_family'] = feature_df['extension_family'].fillna('').astype(str)\n",
    "feature_df['original_path_keywords'] = feature_df['original_path_keywords'].fillna('').astype(str)\n",
    "feature_df['business_capability'] = feature_df['business_capability'].fillna('').astype(str)\n",
    "feature_df['original_path_depth'] = pd.to_numeric(feature_df['original_path_depth'], errors='coerce').fillna(0).astype(int)\n",
    "feature_df['file_size_bytes'] = pd.to_numeric(feature_df['file_size_bytes'], errors='coerce').fillna(0).astype(float)\n",
    "\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16126a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.to_csv(CURATED_DATA_PATH, index=False)\n",
    "print(f'Saved curated dataset to {CURATED_DATA_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056349dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df = feature_df[FEATURE_COLUMNS + [TARGET_COLUMN]].copy()\n",
    "modeling_df = modeling_df[modeling_df[TARGET_COLUMN].str.strip() != '']\n",
    "\n",
    "for column in [TEXT_FEATURE] + CATEGORICAL_FEATURES:\n",
    "    modeling_df[column] = modeling_df[column].fillna('').astype(str)\n",
    "\n",
    "for column in NUMERIC_FEATURES:\n",
    "    modeling_df[column] = pd.to_numeric(modeling_df[column], errors='coerce').fillna(0.0).astype(float)\n",
    "\n",
    "print('Class distribution:')\n",
    "print(modeling_df[TARGET_COLUMN].value_counts())\n",
    "modeling_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4268c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    modeling_df,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=modeling_df[TARGET_COLUMN],\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "train_df.to_csv(TRAIN_DATA_PATH, index=False)\n",
    "test_df.to_csv(TEST_DATA_PATH, index=False)\n",
    "\n",
    "print(f'Train rows: {len(train_df)} | Test rows: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=1,\n",
    "    strip_accents='unicode',\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('path_tfidf', text_vectorizer, TEXT_FEATURE),\n",
    "        ('categorical', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES),\n",
    "        ('numeric', 'passthrough', NUMERIC_FEATURES),\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    sparse_threshold=0.3,\n",
    ")\n",
    "\n",
    "preprocessor_path = OUTPUT_DIR / 'preprocessor_metadata_only.joblib'\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "print(f'Saved preprocessing template to {preprocessor_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d23ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df[FEATURE_COLUMNS]\n",
    "    y_train = train_df[TARGET_COLUMN]\n",
    "    X_test = test_df[FEATURE_COLUMNS]\n",
    "    y_test = test_df[TARGET_COLUMN]\n",
    "\n",
    "    candidate_models = {\n",
    "        'lightgbm': LGBMClassifier(\n",
    "            n_estimators=400,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=64,\n",
    "            objective='multiclass',\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "        ),\n",
    "        'xgboost': XGBClassifier(\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=9,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            objective='multi:softprob',\n",
    "            eval_metric='mlogloss',\n",
    "            random_state=RANDOM_STATE,\n",
    "            tree_method='hist',\n",
    "            n_jobs=-1,\n",
    "            use_label_encoder=False,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    min_class_count = int(y_train.value_counts().min())\n",
    "    effective_folds = max(2, min(CV_FOLDS, min_class_count))\n",
    "    if effective_folds < CV_FOLDS:\n",
    "        print(f'Adjusting CV folds from {CV_FOLDS} to {effective_folds} due to limited samples per class.')\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=effective_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    evaluation_rows = []\n",
    "    model_reports = {}\n",
    "\n",
    "    for model_name, estimator in candidate_models.items():\n",
    "        print(f'\n",
    "Training model: {model_name}')\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', clone(preprocessor)),\n",
    "            ('classifier', estimator),\n",
    "        ])\n",
    "\n",
    "        cv_scores = cross_validate(\n",
    "            pipeline,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=cv,\n",
    "            scoring=['accuracy', 'f1_macro', 'f1_weighted'],\n",
    "            n_jobs=-1,\n",
    "            return_train_score=False,\n",
    "        )\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "\n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        model_reports[model_name] = report\n",
    "\n",
    "        evaluation_rows.append({\n",
    "            'model': model_name,\n",
    "            'cv_accuracy_mean': cv_scores['test_accuracy'].mean(),\n",
    "            'cv_accuracy_std': cv_scores['test_accuracy'].std(),\n",
    "            'cv_macro_f1_mean': cv_scores['test_f1_macro'].mean(),\n",
    "            'cv_weighted_f1_mean': cv_scores['test_f1_weighted'].mean(),\n",
    "            'test_accuracy': accuracy_score(y_test, y_pred),\n",
    "            'test_macro_f1': report['macro avg']['f1-score'],\n",
    "            'test_weighted_f1': report['weighted avg']['f1-score'],\n",
    "        })\n",
    "\n",
    "        joblib.dump(pipeline, MODEL_DIR / f'{model_name}_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118210b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(evaluation_rows).sort_values(by='test_weighted_f1', ascending=False).reset_index(drop=True)\n",
    "results_df.to_csv(MODEL_COMPARISON_PATH, index=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1430469",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = results_df.iloc[0]['model']\n",
    "print(f'Best model based on weighted F1: {best_model_name}')\n",
    "\n",
    "best_report = pd.DataFrame(model_reports[best_model_name]).T\n",
    "display(best_report)\n",
    "\n",
    "BEST_MODEL_PATH = MODEL_DIR / f'{best_model_name}_pipeline.joblib'\n",
    "print(f'Saved best model pipeline to {BEST_MODEL_PATH}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}