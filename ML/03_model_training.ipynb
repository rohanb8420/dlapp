{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf0d906f",
   "metadata": {},
   "source": [
    "# 03 - Model Training & Evaluation\n",
    "\n",
    "This notebook loads the engineered datasets, composes classical machine learning pipelines, and compares candidate models using consistent metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cebae9c",
   "metadata": {},
   "source": [
    "## How to use\n",
    "1. Confirm that the previous notebooks have generated `train_dataset.csv`, `test_dataset.csv`, and `preprocessor.joblib`.\n",
    "2. Adjust the model list or evaluation settings if needed.\n",
    "3. Run the notebook to benchmark classical models and persist the best-performing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62649e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install additional modeling dependencies.\n",
    "# !pip install scikit-learn scipy joblib lightgbm xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4b00c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a9a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ARTIFACT_DIR = NOTEBOOK_DIR / \"artifacts\"\n",
    "ARTIFACT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "TRAIN_DATA_PATH = ARTIFACT_DIR / \"train_dataset.csv\"\n",
    "TEST_DATA_PATH = ARTIFACT_DIR / \"test_dataset.csv\"\n",
    "MODEL_DIR = ARTIFACT_DIR / \"models\"\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "MODEL_COMPARISON_PATH = ARTIFACT_DIR / \"model_comparison.csv\"\n",
    "\n",
    "TARGET_COLUMN = \"business_capability\"\n",
    "TEXT_FEATURE = \"original_path_keywords\"\n",
    "CATEGORICAL_FEATURES = [\"extension\", \"extension_family\"]\n",
    "NUMERIC_FEATURES = [\"original_path_depth\", \"file_size_bytes\", \"content_word_count\"]\n",
    "FEATURE_COLUMNS = [TEXT_FEATURE] + CATEGORICAL_FEATURES + NUMERIC_FEATURES\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "CV_FOLDS = 5\n",
    "\n",
    "for required_path in [TRAIN_DATA_PATH, TEST_DATA_PATH]:\n",
    "    if not required_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing artifact: {required_path}. Run prior notebooks first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef191c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIRED_COLUMNS = FEATURE_COLUMNS + [TARGET_COLUMN]\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH)\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "missing_train = [col for col in REQUIRED_COLUMNS if col not in train_df.columns]\n",
    "missing_test = [col for col in REQUIRED_COLUMNS if col not in test_df.columns]\n",
    "if missing_train or missing_test:\n",
    "    raise ValueError(f\"Missing required columns. train missing={missing_train}, test missing={missing_test}\")\n",
    "\n",
    "def prep_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[TEXT_FEATURE] = df[TEXT_FEATURE].fillna(\"\").astype(str)\n",
    "    for column in CATEGORICAL_FEATURES:\n",
    "        df[column] = df[column].fillna(\"\").astype(str)\n",
    "    for column in NUMERIC_FEATURES:\n",
    "        df[column] = pd.to_numeric(df[column], errors=\"coerce\").fillna(0.0).astype(float)\n",
    "    return df\n",
    "\n",
    "train_df = prep_dataframe(train_df)\n",
    "test_df = prep_dataframe(test_df)\n",
    "\n",
    "X_train = train_df[FEATURE_COLUMNS]\n",
    "y_train = train_df[TARGET_COLUMN]\n",
    "X_test = test_df[FEATURE_COLUMNS]\n",
    "y_test = test_df[TARGET_COLUMN]\n",
    "\n",
    "text_non_empty = int((X_train[TEXT_FEATURE].str.strip() != \"\").sum())\n",
    "print(f\"Non-empty rows for {TEXT_FEATURE}: {text_non_empty}\")\n",
    "print(f\"Unique extensions: {X_train['extension'].nunique()}\")\n",
    "print(f\"Unique extension families: {X_train['extension_family'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1113110",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=1,\n",
    "    strip_accents=\"unicode\",\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"path_tfidf\", text_vectorizer, [TEXT_FEATURE]),\n",
    "        (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\"), CATEGORICAL_FEATURES),\n",
    "        (\"numeric\", \"passthrough\", NUMERIC_FEATURES),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c75e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_models = {\n",
    "    \"lightgbm\": LGBMClassifier(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=64,\n",
    "        objective=\"multiclass\",\n",
    "        random_state=RANDOM_STATE,\n",
    "    ),\n",
    "    \"xgboost\": XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=9,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        objective=\"multi:softprob\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "        random_state=RANDOM_STATE,\n",
    "        tree_method=\"hist\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "evaluation_rows = []\n",
    "model_reports = {}\n",
    "\n",
    "for model_name, estimator in candidate_models.items():\n",
    "    print(f\"\\nTraining model: {model_name}\")\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", clone(preprocessor)),\n",
    "            (\"classifier\", estimator),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    cv_scores = cross_validate(\n",
    "        pipeline,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=cv,\n",
    "        scoring=[\"accuracy\", \"f1_macro\", \"f1_weighted\"],\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False,\n",
    "    )\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "    model_reports[model_name] = report\n",
    "\n",
    "    evaluation_rows.append(\n",
    "        {\n",
    "            \"model\": model_name,\n",
    "            \"cv_accuracy_mean\": cv_scores[\"test_accuracy\"].mean(),\n",
    "            \"cv_accuracy_std\": cv_scores[\"test_accuracy\"].std(),\n",
    "            \"cv_macro_f1_mean\": cv_scores[\"test_f1_macro\"].mean(),\n",
    "            \"cv_weighted_f1_mean\": cv_scores[\"test_f1_weighted\"].mean(),\n",
    "            \"test_accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"test_macro_f1\": report[\"macro avg\"][\"f1-score\"],\n",
    "            \"test_weighted_f1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    joblib.dump(pipeline, MODEL_DIR / f\"{model_name}_pipeline.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6663843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(evaluation_rows).sort_values(by=\"test_weighted_f1\", ascending=False).reset_index(drop=True)\n",
    "results_df.to_csv(MODEL_COMPARISON_PATH, index=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98da506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = results_df.iloc[0][\"model\"]\n",
    "print(f\"Best model based on weighted F1: {best_model_name}\")\n",
    "\n",
    "best_report = pd.DataFrame(model_reports[best_model_name]).T\n",
    "display(best_report)\n",
    "\n",
    "BEST_MODEL_PATH = MODEL_DIR / f\"{best_model_name}_pipeline.joblib\"\n",
    "print(f\"Saved best model pipeline to {BEST_MODEL_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}