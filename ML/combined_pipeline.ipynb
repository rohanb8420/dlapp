{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f097586d",
   "metadata": {},
   "source": [
    "# Data Lifecycle Classification Pipeline\n",
    "\n",
    "End-to-end workflow that ingests the lifecycle catalog, engineers features, and trains classical gradient boosting models (LightGBM & XGBoost) without mutating existing artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac2dc24",
   "metadata": {},
   "source": [
    "## Notebook Outline\n",
    "- Configure paths and dependencies\n",
    "- Ingest the catalog (Excel) and derive per-file metadata\n",
    "- Build the modeling dataset (selected features only)\n",
    "- Train/test split and preprocessing pipeline\n",
    "- Benchmark LightGBM and XGBoost classifiers\n",
    "- Persist optional artifacts (new filenames to avoid overwriting `metadata_features.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install dependencies before first run.\n",
    "# !pip install pandas openpyxl python-magic-bin python-docx PyPDF2 python-pptx extract-msg textract tqdm pyxlsb scikit-learn lightgbm xgboost joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a52b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "try:\n",
    "    import magic  # type: ignore\n",
    "except ImportError:\n",
    "    magic = None\n",
    "\n",
    "try:\n",
    "    from docx import Document  # type: ignore\n",
    "except ImportError:\n",
    "    Document = None\n",
    "\n",
    "try:\n",
    "    from pptx import Presentation  # type: ignore\n",
    "except ImportError:\n",
    "    Presentation = None\n",
    "\n",
    "try:\n",
    "    from PyPDF2 import PdfReader  # type: ignore\n",
    "except ImportError:\n",
    "    PdfReader = None\n",
    "\n",
    "try:\n",
    "    import extract_msg  # type: ignore\n",
    "except ImportError:\n",
    "    extract_msg = None\n",
    "\n",
    "try:\n",
    "    import textract  # type: ignore\n",
    "except ImportError:\n",
    "    textract = None\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm  # type: ignore\n",
    "    TQDM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TQDM_AVAILABLE = False\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a05c193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(transform_output='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e579491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "EXCEL_PATH = PROJECT_ROOT / 'assets' / 'training_data.xlsx'\n",
    "RAW_BASE_DIR = PROJECT_ROOT\n",
    "\n",
    "OUTPUT_DIR = Path.cwd() / 'artifacts_combined'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CURATED_DATA_PATH = OUTPUT_DIR / 'metadata_features_combined.csv'\n",
    "TRAIN_DATA_PATH = OUTPUT_DIR / 'train_dataset_combined.csv'\n",
    "TEST_DATA_PATH = OUTPUT_DIR / 'test_dataset_combined.csv'\n",
    "MODEL_DIR = OUTPUT_DIR / 'models'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_COMPARISON_PATH = OUTPUT_DIR / 'model_comparison.csv'\n",
    "\n",
    "TARGET_COLUMN = 'business_capability'\n",
    "TEXT_FEATURE = 'original_path_keywords'\n",
    "CATEGORICAL_FEATURES = ['extension', 'extension_family']\n",
    "NUMERIC_FEATURES = ['original_path_depth', 'file_size_bytes', 'content_word_count']\n",
    "FEATURE_COLUMNS = [TEXT_FEATURE] + CATEGORICAL_FEATURES + NUMERIC_FEATURES\n",
    "\n",
    "REQUIRED_COLUMNS = ['Original File Path', 'File Path', 'Business Capability']\n",
    "OPTIONAL_COLUMNS = ['Record Type', 'Retention Code', 'Notes']\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "CV_FOLDS = 3  # Reduced to handle rare classes\n",
    "\n",
    "print(f\"Excel catalog path: {EXCEL_PATH}\")\n",
    "print(f\"Artifacts directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee7607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RENAME_MAP = {\n",
    "    'Original File Path': 'original_file_path',\n",
    "    'File Path': 'file_path',\n",
    "    'Business Capability': 'business_capability',\n",
    "    'Record Type': 'record_type',\n",
    "    'Retention Code': 'retention_code',\n",
    "    'Notes': 'notes_text',\n",
    "}\n",
    "\n",
    "\n",
    "def load_catalog(path: Path, required_columns: Iterable[str], optional_columns: Iterable[str]) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'Catalog not found: {path}')\n",
    "    df = pd.read_excel(path)\n",
    "    missing = [col for col in required_columns if col not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f'Missing required columns in catalog: {missing}')\n",
    "    available_optional = [col for col in optional_columns if col in df.columns]\n",
    "    df = df[list(required_columns) + available_optional].copy()\n",
    "    df = df.rename(columns=RENAME_MAP)\n",
    "    for column in ['original_file_path', 'file_path', 'business_capability']:\n",
    "        df[column] = df[column].astype(str).str.strip()\n",
    "    for column in ['record_type', 'retention_code', 'notes_text']:\n",
    "        if column not in df.columns:\n",
    "            df[column] = ''\n",
    "        df[column] = df[column].fillna('').astype(str).str.strip()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5527f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORD_CLEANER = re.compile(r'[^A-Za-z0-9]+')\n",
    "\n",
    "\n",
    "def safe_path(value: str | float | int | None) -> str:\n",
    "    if value is None:\n",
    "        return ''\n",
    "    if isinstance(value, float) and np.isnan(value):\n",
    "        return ''\n",
    "    return str(value).strip()\n",
    "\n",
    "\n",
    "def resolve_path(raw_value: str, base_dir: Path) -> Path:\n",
    "    candidate_str = safe_path(raw_value)\n",
    "    if not candidate_str:\n",
    "        return base_dir\n",
    "    try:\n",
    "        candidate = Path(candidate_str)\n",
    "    except Exception:\n",
    "        candidate = base_dir / candidate_str\n",
    "    if not candidate.is_absolute():\n",
    "        candidate = base_dir / candidate\n",
    "    try:\n",
    "        return candidate.resolve(strict=False)\n",
    "    except Exception:\n",
    "        return candidate\n",
    "\n",
    "\n",
    "def to_keywords_from_path(path: Path) -> str:\n",
    "    tokens = []\n",
    "    for part in path.parts:\n",
    "        cleaned = KEYWORD_CLEANER.sub(' ', part).strip().lower()\n",
    "        if cleaned:\n",
    "            tokens.append(cleaned)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def path_depth(path: Path) -> int:\n",
    "    return len(path.parts)\n",
    "\n",
    "\n",
    "EXTENSION_FAMILY_MAP = {\n",
    "    '.xls': 'excel',\n",
    "    '.xlsx': 'excel',\n",
    "    '.xlsm': 'excel',\n",
    "    '.xlsb': 'excel',\n",
    "    '.csv': 'tabular',\n",
    "    '.tsv': 'tabular',\n",
    "    '.txt': 'text',\n",
    "    '.log': 'text',\n",
    "    '.json': 'json',\n",
    "    '.xml': 'markup',\n",
    "    '.yaml': 'markup',\n",
    "    '.yml': 'markup',\n",
    "    '.ini': 'config',\n",
    "    '.cfg': 'config',\n",
    "    '.conf': 'config',\n",
    "    '.doc': 'word',\n",
    "    '.docx': 'word',\n",
    "    '.pdf': 'pdf',\n",
    "    '.ppt': 'presentation',\n",
    "    '.pptx': 'presentation',\n",
    "    '.msg': 'outlook',\n",
    "    '.html': 'html',\n",
    "    '.htm': 'html',\n",
    "    '.css': 'code',\n",
    "    '.js': 'code',\n",
    "    '.sql': 'code',\n",
    "    '.py': 'code',\n",
    "    '.sas': 'sas',\n",
    "    '.sas7bdat': 'sas',\n",
    "    '.ipynb': 'notebook',\n",
    "    '.jpg': 'image',\n",
    "    '.jpeg': 'image',\n",
    "    '.png': 'image',\n",
    "    '.gif': 'image',\n",
    "    '.bmp': 'image',\n",
    "    '.vsd': 'visio',\n",
    "    '.vsdx': 'visio',\n",
    "    '.twb': 'tableau',\n",
    "    '.twbx': 'tableau',\n",
    "}\n",
    "\n",
    "\n",
    "def extension_family(suffix: str) -> str:\n",
    "    return EXTENSION_FAMILY_MAP.get(suffix.lower(), 'other')\n",
    "\n",
    "\n",
    "def gather_file_stats(path: Path) -> tuple[int, float]:\n",
    "    try:\n",
    "        stat_result = path.stat()\n",
    "        return stat_result.st_size, float(stat_result.st_mtime)\n",
    "    except OSError:\n",
    "        return 0, float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6490c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_EXTENSIONS = {\n",
    "        '.txt', '.csv', '.tsv', '.json', '.xml', '.yaml', '.yml', '.ini', '.cfg', '.conf',\n",
    "        '.md', '.rst', '.sql', '.py', '.java', '.js', '.html', '.htm', '.css', '.log', '.sas', '.psv'\n",
    "    }\n",
    "    EXCEL_EXTENSIONS = {'.xls', '.xlsx', '.xlsm', '.xlsb'}\n",
    "    HTML_EXTENSIONS = {'.html', '.htm'}\n",
    "    MAX_CHAR_LENGTH = 5000\n",
    "\n",
    "\n",
    "    def read_text_file(path: Path) -> str:\n",
    "        for encoding in ('utf-8', 'utf-16', 'latin-1', 'cp1252'):\n",
    "            try:\n",
    "                return path.read_text(encoding=encoding, errors='ignore')\n",
    "            except Exception:\n",
    "                continue\n",
    "        return ''\n",
    "\n",
    "\n",
    "    def normalize_text(text: str) -> str:\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "    def strip_html_tags(text: str) -> str:\n",
    "        text = re.sub(r'<script.*?>.*?</script>', ' ', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        text = re.sub(r'<style.*?>.*?</style>', ' ', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        text = re.sub(r'<[^>]+>', ' ', text)\n",
    "        return normalize_text(text)\n",
    "\n",
    "\n",
    "    def extract_text_from_msg(path: Path) -> str:\n",
    "        if extract_msg is None:\n",
    "            return ''\n",
    "        try:\n",
    "            message = extract_msg.Message(str(path))\n",
    "            sections = [message.subject or '', message.body or '']\n",
    "            attachments = getattr(message, 'attachments', [])\n",
    "            for attachment in attachments:\n",
    "                filename = getattr(attachment, 'longFilename', '') or getattr(attachment, 'filename', '')\n",
    "                if filename:\n",
    "                    sections.append(filename)\n",
    "            return normalize_text(' '.join(section for section in sections if section))\n",
    "        except Exception:\n",
    "            return ''\n",
    "\n",
    "\n",
    "    def excel_to_text(path: Path, suffix: str) -> str:\n",
    "        read_kwargs = dict(sheet_name=None, dtype=str, nrows=200)\n",
    "        if suffix == '.xlsb':\n",
    "            read_kwargs['engine'] = 'pyxlsb'\n",
    "        text_chunks = []\n",
    "        try:\n",
    "            sheets = pd.read_excel(path, **read_kwargs)\n",
    "        except Exception:\n",
    "            return ''\n",
    "        for sheet_name, sheet_df in sheets.items():\n",
    "            text_chunks.append(f'__sheet__: {sheet_name}')\n",
    "            for _, row in sheet_df.iterrows():\n",
    "                row_text = ' '.join(str(value) for value in row if pd.notna(value))\n",
    "                if row_text:\n",
    "                    text_chunks.append(row_text)\n",
    "        return '\n",
    "'.join(text_chunks)\n",
    "\n",
    "\n",
    "    def extract_text_from_file(path: Path) -> str:\n",
    "        if not path.exists() or not path.is_file():\n",
    "            return ''\n",
    "        suffix = path.suffix.lower()\n",
    "        text_content = ''\n",
    "        try:\n",
    "            if suffix in TEXT_EXTENSIONS:\n",
    "                text_content = read_text_file(path)\n",
    "                if suffix in HTML_EXTENSIONS:\n",
    "                    text_content = strip_html_tags(text_content)\n",
    "            elif suffix in EXCEL_EXTENSIONS:\n",
    "                text_content = excel_to_text(path, suffix)\n",
    "            elif suffix == '.pdf' and PdfReader is not None:\n",
    "                try:\n",
    "                    reader = PdfReader(str(path))\n",
    "                    texts = []\n",
    "                    total_len = 0\n",
    "                    for page in reader.pages:\n",
    "                        try:\n",
    "                            page_text = page.extract_text() or ''\n",
    "                        except Exception:\n",
    "                            page_text = ''\n",
    "                        if page_text:\n",
    "                            texts.append(page_text)\n",
    "                            total_len += len(page_text)\n",
    "                            if total_len >= MAX_CHAR_LENGTH:\n",
    "                                break\n",
    "                    text_content = '\n",
    "'.join(texts)\n",
    "                except Exception:\n",
    "                    text_content = ''\n",
    "            elif suffix == '.docx' and Document is not None:\n",
    "                try:\n",
    "                    document = Document(path)\n",
    "                    paragraphs = [paragraph.text for paragraph in document.paragraphs if paragraph.text]\n",
    "                    text_content = '\n",
    "'.join(paragraphs)\n",
    "                except Exception:\n",
    "                    text_content = ''\n",
    "            elif suffix == '.pptx' and Presentation is not None:\n",
    "                try:\n",
    "                    presentation = Presentation(path)\n",
    "                    texts = []\n",
    "                    for slide in presentation.slides:\n",
    "                        for shape in slide.shapes:\n",
    "                            if hasattr(shape, 'text') and shape.text:\n",
    "                                texts.append(shape.text)\n",
    "                    text_content = '\n",
    "'.join(texts)\n",
    "                except Exception:\n",
    "                    text_content = ''\n",
    "            elif suffix == '.msg':\n",
    "                text_content = extract_text_from_msg(path)\n",
    "            elif textract is not None and suffix in {'.doc', '.ppt'}:\n",
    "                try:\n",
    "                    text_content = textract.process(str(path)).decode('utf-8', errors='ignore')\n",
    "                except Exception:\n",
    "                    text_content = ''\n",
    "            else:\n",
    "                if magic is not None:\n",
    "                    try:\n",
    "                        mime_type = magic.from_file(str(path), mime=True)\n",
    "                    except Exception:\n",
    "                        mime_type = None\n",
    "                    if mime_type and 'text' in mime_type:\n",
    "                        text_content = read_text_file(path)\n",
    "                if not text_content and textract is not None and suffix not in {'.dll', ''}:\n",
    "                    try:\n",
    "                        text_content = textract.process(str(path)).decode('utf-8', errors='ignore')\n",
    "                    except Exception:\n",
    "                        text_content = ''\n",
    "                if not text_content:\n",
    "                    text_content = read_text_file(path)\n",
    "        except Exception:\n",
    "            text_content = ''\n",
    "        text_content = normalize_text(text_content)\n",
    "        if suffix in HTML_EXTENSIONS:\n",
    "            text_content = strip_html_tags(text_content)\n",
    "        return text_content[:MAX_CHAR_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a5a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_illegal_characters(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove illegal or non-printable characters from all string columns in a DataFrame.\n",
    "    \"\"\"\n",
    "    illegal_chars = [\n",
    "        '\\x00', '\\x01', '\\x02', '\\x03', '\\x04', '\\x05',\n",
    "        '\\x06', '\\x07', '\\x08', '\\x0b', '\\x0c', '\\x0e',\n",
    "        '\\x0f', '\\x10', '\\x11', '\\x12', '\\x13', '\\x14',\n",
    "        '\\x15', '\\x16', '\\x17', '\\x18', '\\x19', '\\x1a'\n",
    "    ]\n",
    "\n",
    "    def clean_value(x):\n",
    "        if pd.isnull(x):\n",
    "            return x\n",
    "        return ''.join(c for c in str(x) if c.isprintable() and c not in illegal_chars)\n",
    "\n",
    "    str_cols = df.select_dtypes(include=['object']).columns\n",
    "    df[str_cols] = df[str_cols].applymap(clean_value)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2af5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_df = load_catalog(EXCEL_PATH, REQUIRED_COLUMNS, OPTIONAL_COLUMNS)\n",
    "\n",
    "catalog_df['resolved_path'] = catalog_df['file_path'].apply(lambda value: resolve_path(value, RAW_BASE_DIR))\n",
    "catalog_df['original_resolved_path'] = catalog_df['original_file_path'].apply(lambda value: resolve_path(value, RAW_BASE_DIR))\n",
    "catalog_df['extension'] = catalog_df['resolved_path'].apply(lambda p: p.suffix.lower())\n",
    "catalog_df['extension_family'] = catalog_df['extension'].apply(extension_family)\n",
    "catalog_df['file_exists'] = catalog_df['resolved_path'].apply(Path.exists)\n",
    "catalog_df['original_path_depth'] = catalog_df['original_resolved_path'].apply(path_depth)\n",
    "catalog_df['original_path_keywords'] = catalog_df['original_resolved_path'].apply(to_keywords_from_path)\n",
    "\n",
    "size_mtime = catalog_df['resolved_path'].apply(gather_file_stats)\n",
    "catalog_df['file_size_bytes'] = [pair[0] for pair in size_mtime]\n",
    "catalog_df['modified_time_epoch'] = [pair[1] for pair in size_mtime]\n",
    "\n",
    "if TQDM_AVAILABLE:\n",
    "    tqdm.pandas(desc='Extracting content')\n",
    "    catalog_df['content_text'] = catalog_df['resolved_path'].progress_apply(extract_text_from_file)\n",
    "else:\n",
    "    catalog_df['content_text'] = catalog_df['resolved_path'].apply(extract_text_from_file)\n",
    "\n",
    "catalog_df['content_word_count'] = catalog_df['content_text'].str.split().map(len).fillna(0)\n",
    "\n",
    "catalog_df.head()\n",
    "\n",
    "catalog_df = remove_illegal_characters(catalog_df)\n",
    "cleaned_excel_path = OUTPUT_DIR / 'input_data_cleaned.xlsx'\n",
    "try:\n",
    "    catalog_df.to_excel(cleaned_excel_path, index=False, engine='openpyxl')\n",
    "    print(f'Saved cleaned catalog to {cleaned_excel_path}')\n",
    "except Exception as excel_error:\n",
    "    print('Excel export skipped:', excel_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d1b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = catalog_df[\n",
    "    [\n",
    "        'original_file_path',\n",
    "        'file_path',\n",
    "        'business_capability',\n",
    "        'original_resolved_path',\n",
    "        'resolved_path',\n",
    "        'extension',\n",
    "        'extension_family',\n",
    "        'original_path_depth',\n",
    "        'file_size_bytes',\n",
    "        'content_word_count',\n",
    "        'content_text',\n",
    "        'file_exists',\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "feature_df['original_path_keywords'] = catalog_df['original_path_keywords']\n",
    "feature_df['extension'] = feature_df['extension'].fillna('').astype(str)\n",
    "feature_df['extension_family'] = feature_df['extension_family'].fillna('').astype(str)\n",
    "feature_df['original_path_keywords'] = feature_df['original_path_keywords'].fillna('').astype(str)\n",
    "feature_df['business_capability'] = feature_df['business_capability'].fillna('').astype(str)\n",
    "feature_df['original_path_depth'] = pd.to_numeric(feature_df['original_path_depth'], errors='coerce').fillna(0).astype(int)\n",
    "feature_df['file_size_bytes'] = pd.to_numeric(feature_df['file_size_bytes'], errors='coerce').fillna(0).astype(float)\n",
    "feature_df['content_word_count'] = pd.to_numeric(feature_df['content_word_count'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "feature_preview = feature_df[[\n",
    "    'original_path_keywords',\n",
    "    'extension',\n",
    "    'extension_family',\n",
    "    'original_path_depth',\n",
    "    'file_size_bytes',\n",
    "    'content_word_count',\n",
    "    'business_capability',\n",
    "]].head()\n",
    "feature_preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726b9a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: persist curated features to a new file (does not touch existing metadata_features.csv)\n",
    "feature_df.to_csv(CURATED_DATA_PATH, index=False)\n",
    "print(f'Saved curated dataset to {CURATED_DATA_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5431cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df = feature_df[FEATURE_COLUMNS + [TARGET_COLUMN]].copy()\n",
    "\n",
    "modeling_df = modeling_df[modeling_df[TARGET_COLUMN].str.strip() != '']\n",
    "modeling_df = modeling_df.dropna(subset=[TARGET_COLUMN])\n",
    "\n",
    "for column in CATEGORICAL_FEATURES + [TEXT_FEATURE]:\n",
    "    modeling_df[column] = modeling_df[column].fillna('').astype(str)\n",
    "\n",
    "for column in NUMERIC_FEATURES:\n",
    "    modeling_df[column] = pd.to_numeric(modeling_df[column], errors='coerce').fillna(0.0).astype(float)\n",
    "\n",
    "print('Class distribution:')\n",
    "print(modeling_df[TARGET_COLUMN].value_counts())\n",
    "modeling_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90632612",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    modeling_df,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=modeling_df[TARGET_COLUMN],\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "train_df.to_csv(TRAIN_DATA_PATH, index=False)\n",
    "test_df.to_csv(TEST_DATA_PATH, index=False)\n",
    "\n",
    "print(f'Train rows: {len(train_df)} | Test rows: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb73f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=1,\n",
    "    strip_accents='unicode',\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('path_tfidf', text_vectorizer, TEXT_FEATURE),\n",
    "        ('categorical', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES),\n",
    "        ('numeric', 'passthrough', NUMERIC_FEATURES),\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    sparse_threshold=0.3,\n",
    ")\n",
    "\n",
    "preprocessor_path = OUTPUT_DIR / 'preprocessor_combined.joblib'\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "print(f'Saved preprocessing template to {preprocessor_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5b24d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df[FEATURE_COLUMNS]\n",
    "    y_train = train_df[TARGET_COLUMN]\n",
    "    X_test = test_df[FEATURE_COLUMNS]\n",
    "    y_test = test_df[TARGET_COLUMN]\n",
    "\n",
    "    candidate_models = {\n",
    "        'lightgbm': __import__('lightgbm').LGBMClassifier(\n",
    "            n_estimators=400,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=64,\n",
    "            objective='multiclass',\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1,\n",
    "        ),\n",
    "        'xgboost': __import__('xgboost').XGBClassifier(\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=9,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            objective='multi:softprob',\n",
    "            eval_metric='mlogloss',\n",
    "            random_state=RANDOM_STATE,\n",
    "            tree_method='hist',\n",
    "            n_jobs=-1,\n",
    "            use_label_encoder=False,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    min_class_count = int(y_train.value_counts().min())\n",
    "    effective_folds = max(2, min(CV_FOLDS, min_class_count))\n",
    "    if effective_folds < CV_FOLDS:\n",
    "        print(f'Adjusting CV folds from {CV_FOLDS} to {effective_folds} due to limited samples per class.')\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=effective_folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    evaluation_rows = []\n",
    "    model_reports = {}\n",
    "\n",
    "    for model_name, estimator in candidate_models.items():\n",
    "        print(f'\n",
    "Training model: {model_name}')\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', clone(preprocessor)),\n",
    "            ('classifier', estimator),\n",
    "        ])\n",
    "\n",
    "        cv_scores = cross_validate(\n",
    "            pipeline,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=cv,\n",
    "            scoring=['accuracy', 'f1_macro', 'f1_weighted'],\n",
    "            n_jobs=-1,\n",
    "            return_train_score=False,\n",
    "        )\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "\n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        model_reports[model_name] = report\n",
    "\n",
    "        evaluation_rows.append({\n",
    "            'model': model_name,\n",
    "            'cv_accuracy_mean': cv_scores['test_accuracy'].mean(),\n",
    "            'cv_accuracy_std': cv_scores['test_accuracy'].std(),\n",
    "            'cv_macro_f1_mean': cv_scores['test_f1_macro'].mean(),\n",
    "            'cv_weighted_f1_mean': cv_scores['test_f1_weighted'].mean(),\n",
    "            'test_accuracy': accuracy_score(y_test, y_pred),\n",
    "            'test_macro_f1': report['macro avg']['f1-score'],\n",
    "            'test_weighted_f1': report['weighted avg']['f1-score'],\n",
    "        })\n",
    "\n",
    "        joblib.dump(pipeline, MODEL_DIR / f'{model_name}_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e16e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(evaluation_rows).sort_values(by='test_weighted_f1', ascending=False).reset_index(drop=True)\n",
    "results_df.to_csv(MODEL_COMPARISON_PATH, index=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ed1ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = results_df.iloc[0]['model']\n",
    "print(f'Best model based on weighted F1: {best_model_name}')\n",
    "\n",
    "best_report = pd.DataFrame(model_reports[best_model_name]).T\n",
    "display(best_report)\n",
    "\n",
    "BEST_MODEL_PATH = MODEL_DIR / f'{best_model_name}_pipeline.joblib'\n",
    "print(f'Saved best model pipeline to {BEST_MODEL_PATH}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}