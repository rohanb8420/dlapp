{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f38d84",
   "metadata": {},
   "source": [
    "# 01 - Data Ingestion & Metadata Extraction\n",
    "\n",
    "This notebook ingests the lifecycle catalog, resolves file locations, extracts metadata, and materializes a curated feature table for downstream modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a68a00",
   "metadata": {},
   "source": [
    "## How to use\n",
    "1. Update `EXCEL_PATH` and `RAW_BASE_DIR` in the configuration cell if needed.\n",
    "2. Run the notebook to extract features and write `artifacts/metadata_features.csv`.\n",
    "3. Inspect the preview cells to validate coverage, missing files, and extracted text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3eb914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install dependencies if they are not already available in your environment.\n",
    "# !pip install pandas openpyxl python-magic-bin python-docx pdfminer.six python-pptx tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa1c74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import magic  # type: ignore\n",
    "except ImportError:\n",
    "    magic = None  # optional dependency, used for MIME detection when available\n",
    "\n",
    "try:\n",
    "    from docx import Document  # type: ignore\n",
    "except ImportError:\n",
    "    Document = None\n",
    "\n",
    "try:\n",
    "    from pptx import Presentation  # type: ignore\n",
    "except ImportError:\n",
    "    Presentation = None\n",
    "\n",
    "try:\n",
    "    from pdfminer.high_level import extract_text as pdf_extract_text  # type: ignore\n",
    "except ImportError:\n",
    "    pdf_extract_text = None\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm  # type: ignore\n",
    "    TQDM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TQDM_AVAILABLE = False\n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2adff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "EXCEL_PATH = PROJECT_ROOT / 'assets' / 'data_lifecycle.xlsx'  # TODO: update if the catalog lives elsewhere\n",
    "RAW_BASE_DIR = PROJECT_ROOT  # Base directory to resolve entries from `File Path`\n",
    "OUTPUT_DIR = Path.cwd() / 'artifacts'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DATA_PATH = OUTPUT_DIR / 'metadata_features.csv'\n",
    "\n",
    "CATALOG_COLUMNS = ['Original File Path', 'File Path', 'Business Capability']\n",
    "\n",
    "print(f'Excel catalog path: {EXCEL_PATH}')\n",
    "print(f'File base directory: {RAW_BASE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9666c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_catalog(path: Path, expected_columns: Iterable[str]) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'Catalog not found: {path}')\n",
    "    df = pd.read_excel(path)\n",
    "    missing = [col for col in expected_columns if col not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f'Missing columns in catalog: {missing}')\n",
    "    df = df[list(expected_columns)].copy()\n",
    "    df = df.rename(columns={\n",
    "        'Original File Path': 'original_file_path',\n",
    "        'File Path': 'file_path',\n",
    "        'Business Capability': 'business_capability',\n",
    "    })\n",
    "    df['original_file_path'] = df['original_file_path'].astype(str).str.strip()\n",
    "    df['file_path'] = df['file_path'].astype(str).str.strip()\n",
    "    df['business_capability'] = df['business_capability'].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "catalog_df = load_catalog(EXCEL_PATH, CATALOG_COLUMNS)\n",
    "catalog_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e4b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_path(raw_value: str, base_dir: Path) -> Path:\n",
    "    candidate = Path(raw_value)\n",
    "    if not candidate.is_absolute():\n",
    "        candidate = base_dir / candidate\n",
    "    return candidate.resolve()\n",
    "\n",
    "\n",
    "def to_keywords(path: Path) -> str:\n",
    "    tokens = []\n",
    "    for part in path.parts:\n",
    "        cleaned = re.sub(r'[^A-Za-z0-9]+', ' ', part)\n",
    "        cleaned = cleaned.strip().lower()\n",
    "        if cleaned:\n",
    "            tokens.append(cleaned)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def path_depth(path: Path) -> int:\n",
    "    return len(path.parts)\n",
    "\n",
    "catalog_df['resolved_path'] = catalog_df['file_path'].apply(lambda value: resolve_path(value, RAW_BASE_DIR))\n",
    "catalog_df['original_resolved_path'] = catalog_df['original_file_path'].apply(lambda value: resolve_path(value, RAW_BASE_DIR))\n",
    "catalog_df['file_exists'] = catalog_df['resolved_path'].apply(lambda p: p.exists())\n",
    "catalog_df['extension'] = catalog_df['resolved_path'].apply(lambda p: p.suffix.lower())\n",
    "catalog_df['file_name'] = catalog_df['resolved_path'].apply(lambda p: p.name)\n",
    "catalog_df['file_stem'] = catalog_df['resolved_path'].apply(lambda p: p.stem)\n",
    "catalog_df['path_depth'] = catalog_df['resolved_path'].apply(path_depth)\n",
    "catalog_df['original_path_depth'] = catalog_df['original_resolved_path'].apply(path_depth)\n",
    "catalog_df['path_keywords'] = catalog_df['resolved_path'].apply(to_keywords)\n",
    "catalog_df['original_path_keywords'] = catalog_df['original_resolved_path'].apply(to_keywords)\n",
    "catalog_df['path_token_count'] = catalog_df['path_keywords'].str.split().map(len)\n",
    "\n",
    "catalog_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7842a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_EXTENSIONS = {\n",
    "    '.txt', '.csv', '.tsv', '.json', '.xml', '.yaml', '.yml', '.ini', '.cfg', '.conf',\n",
    "    '.md', '.rst', '.sql', '.py', '.java', '.js', '.html', '.htm', '.css', '.log'\n",
    "}\n",
    "MAX_CHAR_LENGTH = 50000\n",
    "\n",
    "\n",
    "def read_text_file(path: Path) -> str:\n",
    "    try:\n",
    "        return path.read_text(encoding='utf-8', errors='ignore')\n",
    "    except Exception:\n",
    "        try:\n",
    "            return path.read_text(encoding='latin-1', errors='ignore')\n",
    "        except Exception:\n",
    "            return ''\n",
    "\n",
    "\n",
    "def extract_text_from_file(path: Path) -> str:\n",
    "    if not path.exists() or not path.is_file():\n",
    "        return ''\n",
    "\n",
    "    suffix = path.suffix.lower()\n",
    "    text_content = ''\n",
    "\n",
    "    try:\n",
    "        if suffix in TEXT_EXTENSIONS:\n",
    "            text_content = read_text_file(path)\n",
    "\n",
    "        elif suffix in {'.xls', '.xlsx'}:\n",
    "            try:\n",
    "                sheets = pd.read_excel(path, sheet_name=None, dtype=str, nrows=200)\n",
    "                text_chunks = []\n",
    "                for sheet_name, sheet_df in sheets.items():\n",
    "                    text_chunks.append(f'__sheet__: {sheet_name}')\n",
    "                    for _, row in sheet_df.iterrows():\n",
    "                        row_text = ' '.join(str(value) for value in row if pd.notna(value))\n",
    "                        if row_text:\n",
    "                            text_chunks.append(row_text)\n",
    "                text_content = '\n",
    "'.join(text_chunks)\n",
    "            except Exception:\n",
    "                text_content = ''\n",
    "\n",
    "        elif suffix == '.pdf' and pdf_extract_text is not None:\n",
    "            try:\n",
    "                text_content = pdf_extract_text(str(path))\n",
    "            except Exception:\n",
    "                text_content = ''\n",
    "\n",
    "        elif suffix == '.docx' and Document is not None:\n",
    "            try:\n",
    "                document = Document(path)\n",
    "                paragraphs = [paragraph.text for paragraph in document.paragraphs if paragraph.text]\n",
    "                text_content = '\n",
    "'.join(paragraphs)\n",
    "            except Exception:\n",
    "                text_content = ''\n",
    "\n",
    "        elif suffix == '.pptx' and Presentation is not None:\n",
    "            try:\n",
    "                presentation = Presentation(path)\n",
    "                texts = []\n",
    "                for slide in presentation.slides:\n",
    "                    for shape in slide.shapes:\n",
    "                        if hasattr(shape, 'text') and shape.text:\n",
    "                            texts.append(shape.text)\n",
    "                text_content = '\n",
    "'.join(texts)\n",
    "            except Exception:\n",
    "                text_content = ''\n",
    "\n",
    "        else:\n",
    "            # Fallback strategies: try naive text read, optionally MIME-based hints\n",
    "            if suffix not in TEXT_EXTENSIONS and magic is not None:\n",
    "                try:\n",
    "                    mime_type = magic.from_file(str(path), mime=True)\n",
    "                    if mime_type and 'text' in mime_type:\n",
    "                        text_content = read_text_file(path)\n",
    "                except Exception:\n",
    "                    text_content = ''\n",
    "            if not text_content:\n",
    "                text_content = read_text_file(path)\n",
    "\n",
    "    except Exception:\n",
    "        text_content = ''\n",
    "\n",
    "    text_content = re.sub(r'\\s+', ' ', text_content).strip()\n",
    "    return text_content[:MAX_CHAR_LENGTH]\n",
    "\n",
    "\n",
    "if TQDM_AVAILABLE:\n",
    "    from tqdm.auto import tqdm  # type: ignore\n",
    "    tqdm.pandas(desc='Extracting file content')\n",
    "    catalog_df['content_text'] = catalog_df['resolved_path'].progress_apply(extract_text_from_file)\n",
    "else:\n",
    "    catalog_df['content_text'] = catalog_df['resolved_path'].apply(extract_text_from_file)\n",
    "\n",
    "catalog_df['content_char_len'] = catalog_df['content_text'].str.len().fillna(0)\n",
    "catalog_df['content_word_count'] = catalog_df['content_text'].str.split().map(len).fillna(0)\n",
    "\n",
    "catalog_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdc8cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = catalog_df.copy()\n",
    "feature_df['resolved_path'] = feature_df['resolved_path'].astype(str)\n",
    "feature_df['original_resolved_path'] = feature_df['original_resolved_path'].astype(str)\n",
    "feature_df['file_exists'] = feature_df['file_exists'].astype(int)\n",
    "\n",
    "feature_df.to_csv(OUTPUT_DATA_PATH, index=False)\n",
    "print(f'Saved curated dataset to {OUTPUT_DATA_PATH}')\n",
    "\n",
    "try:\n",
    "    parquet_path = OUTPUT_DIR / 'metadata_features.parquet'\n",
    "    feature_df.to_parquet(parquet_path, index=False)\n",
    "    print(f'(Optional) Saved Parquet dataset to {parquet_path}')\n",
    "except Exception as parquet_error:\n",
    "    print('Parquet export skipped:', parquet_error)\n",
    "\n",
    "feature_df[['resolved_path', 'file_exists', 'extension', 'path_depth', 'content_char_len', 'business_capability']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997dc1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df['business_capability'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}