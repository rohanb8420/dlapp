{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb28835",
   "metadata": {},
   "source": [
    "# 01 - Data Ingestion & Metadata Extraction\n",
    "\n",
    "This notebook ingests the lifecycle catalog, resolves file locations, extracts metadata and textual signals, and materializes a feature table for downstream modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4ad3b4",
   "metadata": {},
   "source": [
    "## How to use\n",
    "1. Update `EXCEL_PATH` and `RAW_BASE_DIR` if the catalog or files live elsewhere.\n",
    "2. Run the notebook end-to-end to create `artifacts/metadata_features.csv` (and `.parquet`).\n",
    "3. Review the diagnostics cells to understand missing files, extraction coverage, and class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35549a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: ensure dependencies are installed before running the notebook.\n",
    "# !pip install pandas openpyxl python-magic-bin python-docx PyPDF2 python-pptx extract-msg textract tqdm pyxlsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab06654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import magic  # type: ignore\n",
    "except ImportError:\n",
    "    magic = None\n",
    "\n",
    "try:\n",
    "    from docx import Document  # type: ignore\n",
    "except ImportError:\n",
    "    Document = None\n",
    "\n",
    "try:\n",
    "    from pptx import Presentation  # type: ignore\n",
    "except ImportError:\n",
    "    Presentation = None\n",
    "\n",
    "try:\n",
    "    from PyPDF2 import PdfReader  # type: ignore\n",
    "except ImportError:\n",
    "    PdfReader = None\n",
    "\n",
    "try:\n",
    "    import extract_msg  # type: ignore\n",
    "except ImportError:\n",
    "    extract_msg = None\n",
    "\n",
    "try:\n",
    "    import textract  # type: ignore\n",
    "except ImportError:\n",
    "    textract = None\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm  # type: ignore\n",
    "    TQDM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TQDM_AVAILABLE = False\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98b7a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "EXCEL_PATH = PROJECT_ROOT / \"assets\" / \"training_data.xlsx\"\n",
    "RAW_BASE_DIR = PROJECT_ROOT  # adjust if files live elsewhere\n",
    "OUTPUT_DIR = Path.cwd() / \"artifacts\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DATA_PATH = OUTPUT_DIR / \"metadata_features.csv\"\n",
    "\n",
    "REQUIRED_COLUMNS = [\"Original File Path\", \"File Path\", \"Business Capability\"]\n",
    "OPTIONAL_COLUMNS = [\"Record Type\", \"Retention Code\", \"Notes\"]\n",
    "\n",
    "print(f\"Excel catalog path: {EXCEL_PATH}\")\n",
    "print(f\"File base directory: {RAW_BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RENAME_MAP = {\n",
    "    \"Original File Path\": \"original_file_path\",\n",
    "    \"File Path\": \"file_path\",\n",
    "    \"Business Capability\": \"business_capability\",\n",
    "    \"Record Type\": \"record_type\",\n",
    "    \"Retention Code\": \"retention_code\",\n",
    "    \"Notes\": \"notes_text\",\n",
    "}\n",
    "\n",
    "\n",
    "def load_catalog(path: Path, required_columns: Iterable[str], optional_columns: Iterable[str]) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Catalog not found: {path}\")\n",
    "    df = pd.read_excel(path)\n",
    "    missing = [col for col in required_columns if col not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns in catalog: {missing}\")\n",
    "    available_optional = [col for col in optional_columns if col in df.columns]\n",
    "    use_columns = list(required_columns) + available_optional\n",
    "    df = df[use_columns].copy()\n",
    "    df = df.rename(columns=RENAME_MAP)\n",
    "    for column in [\"original_file_path\", \"file_path\", \"business_capability\"]:\n",
    "        df[column] = df[column].astype(str).str.strip()\n",
    "    for column in [\"record_type\", \"retention_code\", \"notes_text\"]:\n",
    "        if column not in df.columns:\n",
    "            df[column] = \"\"\n",
    "        df[column] = df[column].fillna(\"\").astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "\n",
    "catalog_df = load_catalog(EXCEL_PATH, REQUIRED_COLUMNS, OPTIONAL_COLUMNS)\n",
    "catalog_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172778d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORD_CLEANER = re.compile(r\"[^A-Za-z0-9]+\")\n",
    "\n",
    "\n",
    "def resolve_path(raw_value: str, base_dir: Path) -> Path:\n",
    "    candidate = Path(raw_value)\n",
    "    if not candidate.is_absolute():\n",
    "        candidate = base_dir / candidate\n",
    "    return candidate.resolve()\n",
    "\n",
    "\n",
    "def to_keywords_from_path(path: Path) -> str:\n",
    "    tokens = []\n",
    "    for part in path.parts:\n",
    "        cleaned = KEYWORD_CLEANER.sub(\" \", part).strip().lower()\n",
    "        if cleaned:\n",
    "            tokens.append(cleaned)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def to_keywords_from_text(value: str) -> str:\n",
    "    cleaned = KEYWORD_CLEANER.sub(\" \", value).strip().lower()\n",
    "    return \" \".join(token for token in cleaned.split() if token)\n",
    "\n",
    "\n",
    "def path_depth(path: Path) -> int:\n",
    "    return len(path.parts)\n",
    "\n",
    "\n",
    "def gather_file_stats(path: Path) -> tuple[int, float]:\n",
    "    try:\n",
    "        stat_result = path.stat()\n",
    "        return stat_result.st_size, float(stat_result.st_mtime)\n",
    "    except OSError:\n",
    "        return 0, float(\"nan\")\n",
    "\n",
    "\n",
    "EXTENSION_FAMILY_MAP = {\n",
    "    \".xls\": \"excel\",\n",
    "    \".xlsx\": \"excel\",\n",
    "    \".xlsm\": \"excel\",\n",
    "    \".xlsb\": \"excel\",\n",
    "    \".csv\": \"tabular\",\n",
    "    \".tsv\": \"tabular\",\n",
    "    \".txt\": \"text\",\n",
    "    \".log\": \"text\",\n",
    "    \".json\": \"json\",\n",
    "    \".xml\": \"markup\",\n",
    "    \".yaml\": \"markup\",\n",
    "    \".yml\": \"markup\",\n",
    "    \".ini\": \"config\",\n",
    "    \".cfg\": \"config\",\n",
    "    \".conf\": \"config\",\n",
    "    \".doc\": \"word\",\n",
    "    \".docx\": \"word\",\n",
    "    \".pdf\": \"pdf\",\n",
    "    \".ppt\": \"presentation\",\n",
    "    \".pptx\": \"presentation\",\n",
    "    \".msg\": \"outlook\",\n",
    "    \".html\": \"html\",\n",
    "    \".htm\": \"html\",\n",
    "    \".css\": \"code\",\n",
    "    \".js\": \"code\",\n",
    "    \".sql\": \"code\",\n",
    "    \".py\": \"code\",\n",
    "    \".sas\": \"sas\",\n",
    "    \".sas7bdat\": \"sas\",\n",
    "    \".ipynb\": \"notebook\",\n",
    "    \".jpg\": \"image\",\n",
    "    \".jpeg\": \"image\",\n",
    "    \".png\": \"image\",\n",
    "    \".gif\": \"image\",\n",
    "    \".bmp\": \"image\",\n",
    "    \".vsd\": \"visio\",\n",
    "    \".vsdx\": \"visio\",\n",
    "    \".twb\": \"tableau\",\n",
    "    \".twbx\": \"tableau\",\n",
    "}\n",
    "\n",
    "\n",
    "def extension_family(suffix: str) -> str:\n",
    "    return EXTENSION_FAMILY_MAP.get(suffix.lower(), \"other\")\n",
    "\n",
    "\n",
    "def count_digits(value: str) -> int:\n",
    "    return sum(char.isdigit() for char in value)\n",
    "\n",
    "\n",
    "catalog_df[\"resolved_path\"] = catalog_df[\"file_path\"].apply(lambda value: resolve_path(value, RAW_BASE_DIR))\n",
    "catalog_df[\"original_resolved_path\"] = catalog_df[\"original_file_path\"].apply(lambda value: resolve_path(value, RAW_BASE_DIR))\n",
    "catalog_df[\"file_exists\"] = catalog_df[\"resolved_path\"].apply(Path.exists)\n",
    "catalog_df[\"extension\"] = catalog_df[\"resolved_path\"].apply(lambda p: p.suffix.lower())\n",
    "catalog_df[\"extension_family\"] = catalog_df[\"extension\"].apply(extension_family)\n",
    "catalog_df[\"file_name\"] = catalog_df[\"resolved_path\"].apply(lambda p: p.name)\n",
    "catalog_df[\"file_stem\"] = catalog_df[\"resolved_path\"].apply(lambda p: p.stem)\n",
    "catalog_df[\"file_name_keywords\"] = catalog_df[\"file_name\"].map(to_keywords_from_text)\n",
    "catalog_df[\"file_stem_keywords\"] = catalog_df[\"file_stem\"].map(to_keywords_from_text)\n",
    "catalog_df[\"path_keywords\"] = catalog_df[\"resolved_path\"].apply(to_keywords_from_path)\n",
    "catalog_df[\"original_path_keywords\"] = catalog_df[\"original_resolved_path\"].apply(to_keywords_from_path)\n",
    "catalog_df[\"path_depth\"] = catalog_df[\"resolved_path\"].apply(path_depth)\n",
    "catalog_df[\"original_path_depth\"] = catalog_df[\"original_resolved_path\"].apply(path_depth)\n",
    "catalog_df[\"path_token_count\"] = catalog_df[\"path_keywords\"].str.split().map(len)\n",
    "catalog_df[\"original_path_token_count\"] = catalog_df[\"original_path_keywords\"].str.split().map(len)\n",
    "catalog_df[\"path_char_len\"] = catalog_df[\"resolved_path\"].astype(str).str.len()\n",
    "catalog_df[\"original_path_char_len\"] = catalog_df[\"original_resolved_path\"].astype(str).str.len()\n",
    "catalog_df[\"file_name_char_len\"] = catalog_df[\"file_name\"].str.len()\n",
    "catalog_df[\"file_name_digit_count\"] = catalog_df[\"file_name\"].map(count_digits)\n",
    "\n",
    "size_mtime = catalog_df[\"resolved_path\"].apply(gather_file_stats)\n",
    "catalog_df[\"file_size_bytes\"] = [pair[0] for pair in size_mtime]\n",
    "catalog_df[\"modified_time_epoch\"] = [pair[1] for pair in size_mtime]\n",
    "catalog_df[\"modified_time_iso\"] = pd.to_datetime(catalog_df[\"modified_time_epoch\"], unit=\"s\", errors=\"coerce\")\n",
    "catalog_df[\"notes_word_count\"] = catalog_df[\"notes_text\"].str.split().map(len)\n",
    "catalog_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972fb663",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_EXTENSIONS = {\n",
    "    \".txt\", \".csv\", \".tsv\", \".json\", \".xml\", \".yaml\", \".yml\", \".ini\", \".cfg\", \".conf\",\n",
    "    \".md\", \".rst\", \".sql\", \".py\", \".java\", \".js\", \".html\", \".htm\", \".css\", \".log\", \".sas\", \".psv\"\n",
    "}\n",
    "EXCEL_EXTENSIONS = {\".xls\", \".xlsx\", \".xlsm\", \".xlsb\"}\n",
    "HTML_EXTENSIONS = {\".html\", \".htm\"}\n",
    "MAX_CHAR_LENGTH = 5000\n",
    "\n",
    "\n",
    "def read_text_file(path: Path) -> str:\n",
    "    for encoding in (\"utf-8\", \"utf-16\", \"latin-1\", \"cp1252\"):\n",
    "        try:\n",
    "            return path.read_text(encoding=encoding, errors=\"ignore\")\n",
    "        except Exception:\n",
    "            continue\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "\n",
    "def strip_html_tags(text: str) -> str:\n",
    "    text = re.sub(r\"<script.*?>.*?</script>\", \" \", text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    text = re.sub(r\"<style.*?>.*?</style>\", \" \", text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "    return normalize_text(text)\n",
    "\n",
    "\n",
    "def extract_text_from_msg(path: Path) -> str:\n",
    "    if extract_msg is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        message = extract_msg.Message(str(path))\n",
    "        sections = [message.subject or \"\", message.body or \"\"]\n",
    "        attachments = getattr(message, \"attachments\", [])\n",
    "        for attachment in attachments:\n",
    "            filename = getattr(attachment, \"longFilename\", \"\") or getattr(attachment, \"filename\", \"\")\n",
    "            if filename:\n",
    "                sections.append(filename)\n",
    "        return normalize_text(\" \".join(section for section in sections if section))\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def excel_to_text(path: Path, suffix: str) -> str:\n",
    "    read_kwargs = dict(sheet_name=None, dtype=str, nrows=200)\n",
    "    if suffix == \".xlsb\":\n",
    "        read_kwargs[\"engine\"] = \"pyxlsb\"\n",
    "    text_chunks = []\n",
    "    try:\n",
    "        sheets = pd.read_excel(path, **read_kwargs)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    for sheet_name, sheet_df in sheets.items():\n",
    "        text_chunks.append(f\"__sheet__: {sheet_name}\")\n",
    "        for _, row in sheet_df.iterrows():\n",
    "            row_text = \" \".join(str(value) for value in row if pd.notna(value))\n",
    "            if row_text:\n",
    "                text_chunks.append(row_text)\n",
    "    return \"\\n\".join(text_chunks)\n",
    "\n",
    "\n",
    "def extract_text_from_file(path: Path) -> str:\n",
    "    if not path.exists() or not path.is_file():\n",
    "        return \"\"\n",
    "    suffix = path.suffix.lower()\n",
    "    text_content = \"\"\n",
    "    try:\n",
    "        if suffix in TEXT_EXTENSIONS:\n",
    "            text_content = read_text_file(path)\n",
    "            if suffix in HTML_EXTENSIONS:\n",
    "                text_content = strip_html_tags(text_content)\n",
    "        elif suffix in EXCEL_EXTENSIONS:\n",
    "            text_content = excel_to_text(path, suffix)\n",
    "        elif suffix == \".pdf\" and PdfReader is not None:\n",
    "            try:\n",
    "                reader = PdfReader(str(path))\n",
    "                texts = []\n",
    "                total_len = 0\n",
    "                for page in reader.pages:\n",
    "                    try:\n",
    "                        page_text = page.extract_text() or \"\"\n",
    "                    except Exception:\n",
    "                        page_text = \"\"\n",
    "                    if page_text:\n",
    "                        texts.append(page_text)\n",
    "                        total_len += len(page_text)\n",
    "                        if total_len >= MAX_CHAR_LENGTH:\n",
    "                            break\n",
    "                text_content = \"\\n\".join(texts)\n",
    "            except Exception:\n",
    "                text_content = \"\"\n",
    "        elif suffix == \".docx\" and Document is not None:\n",
    "            try:\n",
    "                document = Document(path)\n",
    "                paragraphs = [paragraph.text for paragraph in document.paragraphs if paragraph.text]\n",
    "                text_content = \"\\n\".join(paragraphs)\n",
    "            except Exception:\n",
    "                text_content = \"\"\n",
    "        elif suffix == \".pptx\" and Presentation is not None:\n",
    "            try:\n",
    "                presentation = Presentation(path)\n",
    "                texts = []\n",
    "                for slide in presentation.slides:\n",
    "                    for shape in slide.shapes:\n",
    "                        if hasattr(shape, \"text\") and shape.text:\n",
    "                            texts.append(shape.text)\n",
    "                text_content = \"\\n\".join(texts)\n",
    "            except Exception:\n",
    "                text_content = \"\"\n",
    "        elif suffix == \".msg\":\n",
    "            text_content = extract_text_from_msg(path)\n",
    "        elif textract is not None and suffix in {\".doc\", \".ppt\"}:\n",
    "            try:\n",
    "                text_content = textract.process(str(path)).decode(\"utf-8\", errors=\"ignore\")\n",
    "            except Exception:\n",
    "                text_content = \"\"\n",
    "        else:\n",
    "            if magic is not None:\n",
    "                try:\n",
    "                    mime_type = magic.from_file(str(path), mime=True)\n",
    "                except Exception:\n",
    "                    mime_type = None\n",
    "                if mime_type and \"text\" in mime_type:\n",
    "                    text_content = read_text_file(path)\n",
    "            if not text_content and textract is not None and suffix not in {\".dll\", \"\"}:\n",
    "                try:\n",
    "                    text_content = textract.process(str(path)).decode(\"utf-8\", errors=\"ignore\")\n",
    "                except Exception:\n",
    "                    text_content = \"\"\n",
    "            if not text_content:\n",
    "                text_content = read_text_file(path)\n",
    "    except Exception:\n",
    "        text_content = \"\"\n",
    "    text_content = normalize_text(text_content)\n",
    "    if suffix in HTML_EXTENSIONS:\n",
    "        text_content = strip_html_tags(text_content)\n",
    "    return text_content[:MAX_CHAR_LENGTH]\n",
    "\n",
    "\n",
    "if TQDM_AVAILABLE:\n",
    "    tqdm.pandas(desc=\"Extracting file content\")\n",
    "    catalog_df[\"content_text\"] = catalog_df[\"resolved_path\"].progress_apply(extract_text_from_file)\n",
    "else:\n",
    "    catalog_df[\"content_text\"] = catalog_df[\"resolved_path\"].apply(extract_text_from_file)\n",
    "\n",
    "catalog_df[\"content_char_len\"] = catalog_df[\"content_text\"].str.len().fillna(0)\n",
    "catalog_df[\"content_word_count\"] = catalog_df[\"content_text\"].str.split().map(len).fillna(0)\n",
    "catalog_df[\"has_content\"] = (catalog_df[\"content_char_len\"] > 0).astype(int)\n",
    "catalog_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c01f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = catalog_df.copy()\n",
    "\n",
    "feature_df[\"resolved_path\"] = feature_df[\"resolved_path\"].astype(str)\n",
    "feature_df[\"original_resolved_path\"] = feature_df[\"original_resolved_path\"].astype(str)\n",
    "feature_df[\"extension\"] = feature_df[\"extension\"].fillna(\"\").astype(str)\n",
    "feature_df[\"extension_family\"] = feature_df[\"extension_family\"].fillna(\"\").astype(str)\n",
    "feature_df[\"record_type\"] = feature_df[\"record_type\"].fillna(\"\").astype(str)\n",
    "feature_df[\"retention_code\"] = feature_df[\"retention_code\"].fillna(\"\").astype(str)\n",
    "feature_df[\"notes_text\"] = feature_df[\"notes_text\"].fillna(\"\").astype(str)\n",
    "feature_df[\"file_name_keywords\"] = feature_df[\"file_name_keywords\"].fillna(\"\").astype(str)\n",
    "feature_df[\"file_stem_keywords\"] = feature_df[\"file_stem_keywords\"].fillna(\"\").astype(str)\n",
    "feature_df[\"path_keywords\"] = feature_df[\"path_keywords\"].fillna(\"\").astype(str)\n",
    "feature_df[\"original_path_keywords\"] = feature_df[\"original_path_keywords\"].fillna(\"\").astype(str)\n",
    "\n",
    "numeric_fields = [\n",
    "    \"file_exists\",\n",
    "    \"file_size_bytes\",\n",
    "    \"modified_time_epoch\",\n",
    "    \"path_depth\",\n",
    "    \"original_path_depth\",\n",
    "    \"path_token_count\",\n",
    "    \"original_path_token_count\",\n",
    "    \"path_char_len\",\n",
    "    \"original_path_char_len\",\n",
    "    \"file_name_char_len\",\n",
    "    \"file_name_digit_count\",\n",
    "    \"content_char_len\",\n",
    "    \"content_word_count\",\n",
    "    \"notes_word_count\",\n",
    "    \"has_content\",\n",
    "]\n",
    "for column in numeric_fields:\n",
    "    if column in feature_df:\n",
    "        feature_df[column] = feature_df[column].fillna(0)\n",
    "\n",
    "feature_df[\"file_exists\"] = feature_df[\"file_exists\"].astype(int)\n",
    "feature_df[\"has_content\"] = feature_df[\"has_content\"].astype(int)\n",
    "\n",
    "feature_df[\"modified_time_iso\"] = pd.to_datetime(feature_df[\"modified_time_iso\"], errors=\"coerce\")\n",
    "feature_df[\"modified_time_iso\"] = feature_df[\"modified_time_iso\"].dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "feature_df[\"modified_time_iso\"] = feature_df[\"modified_time_iso\"].replace(\"NaT\", \"\")\n",
    "\n",
    "feature_df.to_csv(OUTPUT_DATA_PATH, index=False)\n",
    "print(f\"Saved curated dataset to {OUTPUT_DATA_PATH}\")\n",
    "\n",
    "try:\n",
    "    parquet_path = OUTPUT_DIR / \"metadata_features.parquet\"\n",
    "    feature_df.to_parquet(parquet_path, index=False)\n",
    "    print(f\"(Optional) Saved Parquet dataset to {parquet_path}\")\n",
    "except Exception as parquet_error:\n",
    "    print(\"Parquet export skipped:\", parquet_error)\n",
    "\n",
    "feature_df[\n",
    "    [\n",
    "        \"business_capability\",\n",
    "        \"extension\",\n",
    "        \"extension_family\",\n",
    "        \"file_exists\",\n",
    "        \"file_size_bytes\",\n",
    "        \"content_char_len\",\n",
    "        \"notes_word_count\",\n",
    "    ]\n",
    "].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95b612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_df[\"extension\"].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb46878",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_df[\"file_exists\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43338ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_df[\"has_content\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa0a6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_df[\"business_capability\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}