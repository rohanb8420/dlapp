{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59fa186b",
   "metadata": {},
   "source": [
    "# Document Metadata Ingestion Notebook\n",
    "\n",
    "This notebook mirrors the functionality of `reader.py`. Configure the paths in the parameter cell, run the ingestion helper(s), and optionally launch the Streamlit UI directly from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73d3f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import hashlib\n",
    "import io\n",
    "import logging\n",
    "import re\n",
    "import mimetypes\n",
    "import shutil\n",
    "import sqlite3\n",
    "import subprocess\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Sequence\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "try:  # Optional UI dependency\n",
    "    import streamlit as st\n",
    "except Exception:  # pragma: no cover\n",
    "    st = None\n",
    "\n",
    "try:\n",
    "    from docx import Document as DocxDocument\n",
    "except Exception:  # pragma: no cover\n",
    "    DocxDocument = None\n",
    "\n",
    "try:\n",
    "    from pptx import Presentation as PptxPresentation\n",
    "except Exception:  # pragma: no cover\n",
    "    PptxPresentation = None\n",
    "\n",
    "try:\n",
    "    import openpyxl\n",
    "except Exception:  # pragma: no cover\n",
    "    openpyxl = None\n",
    "\n",
    "try:\n",
    "    from PyPDF2 import PdfReader\n",
    "except Exception:  # pragma: no cover\n",
    "    PdfReader = None\n",
    "\n",
    "try:  # Apache Tika (optional)\n",
    "    from tika import parser as tika_parser\n",
    "except Exception:  # pragma: no cover\n",
    "    tika_parser = None\n",
    "\n",
    "LOGGER = logging.getLogger(\"notebook_reader\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ed296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TEXT = 100_000\n",
    "DB_PATH_DEFAULT = Path(\"artifacts\") / \"dlm_reader.db\"\n",
    "\n",
    "FILEPATH_COLUMN = \"filepath\"\n",
    "FILEPATH_ALIASES: Sequence[str] = (\"file_location\", \"file_path\", \"path\")\n",
    "CATEGORY_COLUMN = \"businesscapability\"\n",
    "CATEGORY_ALIASES: Sequence[str] = (\n",
    "    \"business_category\",\n",
    "    \"businessCategory\",\n",
    "    \"BusinessCapability\",\n",
    ")\n",
    "\n",
    "UNREADABLE_EXTENSIONS: Sequence[str] = (\n",
    "    \"png\",\n",
    "    \"jpg\",\n",
    "    \"jpeg\",\n",
    "    \"gif\",\n",
    "    \"bmp\",\n",
    "    \"tif\",\n",
    "    \"tiff\",\n",
    "    \"svg\",\n",
    "    \"webp\",\n",
    "    \"heic\",\n",
    "    \"ico\",\n",
    "    \"accdb\",\n",
    "    \"mdb\",\n",
    "    \"ai\",\n",
    "    \"psd\",\n",
    ")\n",
    "\n",
    "SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS files(\n",
    "  file_id INTEGER PRIMARY KEY,\n",
    "  path TEXT UNIQUE,\n",
    "  folder TEXT,\n",
    "  file_name TEXT,\n",
    "  extension TEXT,\n",
    "  mime_type TEXT,\n",
    "  size_bytes INTEGER,\n",
    "  created_ts TEXT,\n",
    "  modified_ts TEXT,\n",
    "  sha1 TEXT,\n",
    "  exists_flag INTEGER,\n",
    "  read_error TEXT\n",
    ");\n",
    "CREATE TABLE IF NOT EXISTS labels(\n",
    "  file_id INTEGER,\n",
    "  business_category TEXT,\n",
    "  UNIQUE(file_id, business_category),\n",
    "  FOREIGN KEY(file_id) REFERENCES files(file_id)\n",
    ");\n",
    "CREATE TABLE IF NOT EXISTS content(\n",
    "  file_id INTEGER PRIMARY KEY,\n",
    "  content_text TEXT,\n",
    "  FOREIGN KEY(file_id) REFERENCES files(file_id)\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS idx_files_path ON files(path);\n",
    "CREATE INDEX IF NOT EXISTS idx_labels_file ON labels(file_id);\n",
    "\"\"\"\n",
    "\n",
    "ASCII_RE = re.compile(r\"[ -~]{4,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a833c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_db(db_path: Path) -> None:\n",
    "    db_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    con = sqlite3.connect(str(db_path))\n",
    "    with con:\n",
    "        con.executescript(SCHEMA)\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def sha1_of_file(path: Path) -> Optional[str]:\n",
    "    try:\n",
    "        h = hashlib.sha1()\n",
    "        with path.open(\"rb\") as handle:\n",
    "            for chunk in iter(lambda: handle.read(8192), b\"\"):\n",
    "                h.update(chunk)\n",
    "        return h.hexdigest()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def stat_path(path_str: str) -> Dict[str, Any]:\n",
    "    path = Path(path_str)\n",
    "    info: Dict[str, Any] = {\n",
    "        \"path\": str(path),\n",
    "        \"folder\": str(path.parent),\n",
    "        \"file_name\": path.name,\n",
    "        \"extension\": path.suffix.lower().lstrip(\".\"),\n",
    "        \"mime_type\": mimetypes.guess_type(str(path))[0] or \"\",\n",
    "        \"size_bytes\": None,\n",
    "        \"created_ts\": None,\n",
    "        \"modified_ts\": None,\n",
    "        \"sha1\": None,\n",
    "        \"exists_flag\": 0,\n",
    "        \"read_error\": None,\n",
    "    }\n",
    "    try:\n",
    "        st = path.stat()\n",
    "        info.update(\n",
    "            {\n",
    "                \"size_bytes\": int(st.st_size),\n",
    "                \"created_ts\": datetime.fromtimestamp(st.st_ctime).isoformat(),\n",
    "                \"modified_ts\": datetime.fromtimestamp(st.st_mtime).isoformat(),\n",
    "                \"sha1\": sha1_of_file(path),\n",
    "                \"exists_flag\": 1,\n",
    "            }\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        info[\"read_error\"] = f\"stat_error: {exc}\"\n",
    "    return info\n",
    "\n",
    "\n",
    "def _read_txt(path: Path) -> str:\n",
    "    try:\n",
    "        with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as handle:\n",
    "            return handle.read(MAX_TEXT)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _read_csv(path: Path) -> str:\n",
    "    try:\n",
    "        rows: List[str] = []\n",
    "        with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\", newline=\"\") as handle:\n",
    "            reader = csv.reader(handle)\n",
    "            for row in reader:\n",
    "                rows.append(\", \".join(cell.strip() for cell in row if cell))\n",
    "                if len(\"\\n\".join(rows)) > MAX_TEXT:\n",
    "                    break\n",
    "        return \"\\n\".join(rows)[:MAX_TEXT]\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _read_pdf(path: Path) -> str:\n",
    "    if PdfReader is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        reader = PdfReader(str(path))\n",
    "        parts: List[str] = []\n",
    "        for page in reader.pages:\n",
    "            try:\n",
    "                text = page.extract_text() or \"\"\n",
    "            except Exception:\n",
    "                text = \"\"\n",
    "            if text:\n",
    "                parts.append(text)\n",
    "            if len(\"\\n\".join(parts)) > MAX_TEXT:\n",
    "                break\n",
    "        return \"\\n\".join(parts)[:MAX_TEXT]\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _read_docx(path: Path) -> str:\n",
    "    if DocxDocument is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        document = DocxDocument(str(path))\n",
    "        return \"\\n\".join(p.text for p in document.paragraphs if p.text)[:MAX_TEXT]\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _read_pptx(path: Path) -> str:\n",
    "    if PptxPresentation is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        presentation = PptxPresentation(str(path))\n",
    "        pieces: List[str] = []\n",
    "        for slide in presentation.slides:\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, \"text\") and shape.text:\n",
    "                    pieces.append(shape.text)\n",
    "            if len(\"\\n\".join(pieces)) > MAX_TEXT:\n",
    "                break\n",
    "        return \"\\n\".join(pieces)[:MAX_TEXT]\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _read_xlsx(path: Path) -> str:\n",
    "    if openpyxl is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        wb = openpyxl.load_workbook(str(path), read_only=True, data_only=True)\n",
    "        lines: List[str] = []\n",
    "        for sheet in wb.worksheets:\n",
    "            for row in sheet.iter_rows(values_only=True):\n",
    "                vals = [str(cell) for cell in row if cell is not None]\n",
    "                if vals:\n",
    "                    lines.append(\" \".join(vals))\n",
    "                if len(\"\\n\".join(lines)) > MAX_TEXT:\n",
    "                    break\n",
    "            if len(\"\\n\".join(lines)) > MAX_TEXT:\n",
    "                break\n",
    "        return \"\\n\".join(lines)[:MAX_TEXT]\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _text_from_dataframe(df: pd.DataFrame) -> str:\n",
    "    if df.empty:\n",
    "        return \"\"\n",
    "    buffer = io.StringIO()\n",
    "    df.to_csv(buffer, index=False)\n",
    "    return buffer.getvalue()[:MAX_TEXT]\n",
    "\n",
    "\n",
    "def _read_xls(path: Path) -> str:\n",
    "    try:\n",
    "        df = pd.read_excel(str(path), engine=None)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    return _text_from_dataframe(df)\n",
    "\n",
    "\n",
    "def _read_twb(path: Path) -> str:\n",
    "    try:\n",
    "        tree = ET.parse(str(path))\n",
    "        root = tree.getroot()\n",
    "        texts: List[str] = []\n",
    "        for elem in root.iter():\n",
    "            if elem.text and elem.text.strip():\n",
    "                texts.append(elem.text.strip())\n",
    "            for attr in (\"name\", \"caption\", \"label\", \"value\"):\n",
    "                value = elem.attrib.get(attr)\n",
    "                if value:\n",
    "                    texts.append(value)\n",
    "            if len(\"\\n\".join(texts)) > MAX_TEXT:\n",
    "                break\n",
    "        return \"\\n\".join(texts)[:MAX_TEXT]\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _read_twbx(path: Path) -> str:\n",
    "    try:\n",
    "        with zipfile.ZipFile(str(path)) as archive:\n",
    "            members = [m for m in archive.namelist() if m.endswith(\".twb\")]\n",
    "            if not members:\n",
    "                return \"\"\n",
    "            data = archive.read(members[0])\n",
    "            root = ET.fromstring(data)\n",
    "            texts: List[str] = []\n",
    "            for elem in root.iter():\n",
    "                if elem.text and elem.text.strip():\n",
    "                    texts.append(elem.text.strip())\n",
    "                for attr in (\"name\", \"caption\", \"label\", \"value\"):\n",
    "                    value = elem.attrib.get(attr)\n",
    "                    if value:\n",
    "                        texts.append(value)\n",
    "                if len(\"\\n\".join(texts)) > MAX_TEXT:\n",
    "                    break\n",
    "            return \"\\n\".join(texts)[:MAX_TEXT]\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _read_sas_dataset(path: Path) -> str:\n",
    "    try:\n",
    "        df = pd.read_sas(str(path), encoding=\"utf-8\", format=\"sas7bdat\")\n",
    "    except ValueError:\n",
    "        try:\n",
    "            df = pd.read_sas(str(path))\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        return _text_from_dataframe(df)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def _read_dll(path: Path) -> str:\n",
    "    try:\n",
    "        with path.open(\"rb\") as handle:\n",
    "            data = handle.read(min(2_000_000, MAX_TEXT * 4))\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    matches = ASCII_RE.findall(data)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    text = \"\n\".join(chunk.decode(\"ascii\", errors=\"ignore\") for chunk in matches)\n",
    "    return text[:MAX_TEXT]\n",
    "\n",
    "\n",
    "def _read_with_tika(path: Path) -> str:\n",
    "    if tika_parser is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        parsed = tika_parser.from_file(str(path))\n",
    "        return (parsed.get(\"content\") or \"\")[:MAX_TEXT]\n",
    "    except Exception as exc:\n",
    "        LOGGER.debug(\"Tika extraction failed for %s: %s\", path, exc)\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def log_unreadable(path: Path, extension: str) -> None:\n",
    "    LOGGER.info(\"Unreadable format for %s (%s)\", path, extension or \"unknown\")\n",
    "\n",
    "\n",
    "def read_with_tika_and_log(path: Path, extension: str) -> str:\n",
    "    text = _read_with_tika(path)\n",
    "    if not text:\n",
    "        log_unreadable(path, extension)\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_text(path: Path, extension: str) -> str:\n",
    "    ext = (extension or \"\").lower()\n",
    "    if ext in {\"txt\", \"log\", \"md\"}:\n",
    "        return _read_txt(path)\n",
    "    if ext in {\"html\", \"htm\"}:\n",
    "        return _read_txt(path)\n",
    "    if ext == \"csv\":\n",
    "        return _read_csv(path)\n",
    "    if ext == \"pdf\":\n",
    "        text = _read_pdf(path)\n",
    "        return text or read_with_tika_and_log(path, ext)\n",
    "    if ext == \"docx\":\n",
    "        text = _read_docx(path)\n",
    "        return text or read_with_tika_and_log(path, ext)\n",
    "    if ext == \"pptx\":\n",
    "        text = _read_pptx(path)\n",
    "        return text or read_with_tika_and_log(path, ext)\n",
    "    if ext in {\"xlsx\", \"xlsm\"}:\n",
    "        text = _read_xlsx(path)\n",
    "        return text or read_with_tika_and_log(path, ext)\n",
    "    if ext == \"xls\":\n",
    "        text = _read_xls(path)\n",
    "        return text or read_with_tika_and_log(path, ext)\n",
    "    if ext == \"twb\":\n",
    "        return _read_twb(path)\n",
    "    if ext == \"twbx\":\n",
    "        return _read_twbx(path)\n",
    "    if ext == \"sas\":\n",
    "        return _read_txt(path)\n",
    "    if ext in {\"sas7bdat\", \"sasbdat\"}:\n",
    "        text = _read_sas_dataset(path)\n",
    "        return text or read_with_tika_and_log(path, ext)\n",
    "    if ext == \"dll\":\n",
    "        return _read_dll(path)\n",
    "    if ext in {\"one\", \"ppt\", \"doc\", \"msg\"}:\n",
    "        return read_with_tika_and_log(path, ext)\n",
    "    if ext in UNREADABLE_EXTENSIONS:\n",
    "        log_unreadable(path, ext)\n",
    "        return \"\"\n",
    "    text = read_with_tika_and_log(path, ext)\n",
    "    if text:\n",
    "        return text\n",
    "    if not ext:\n",
    "        log_unreadable(path, ext)\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd5ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_file(con: sqlite3.Connection, meta: Dict[str, Any]) -> int:\n",
    "    with con:\n",
    "        con.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO files(path, folder, file_name, extension, mime_type, size_bytes,\n",
    "                              created_ts, modified_ts, sha1, exists_flag, read_error)\n",
    "            VALUES (:path, :folder, :file_name, :extension, :mime_type, :size_bytes,\n",
    "                    :created_ts, :modified_ts, :sha1, :exists_flag, :read_error)\n",
    "            ON CONFLICT(path) DO UPDATE SET\n",
    "                folder=excluded.folder,\n",
    "                file_name=excluded.file_name,\n",
    "                extension=excluded.extension,\n",
    "                mime_type=excluded.mime_type,\n",
    "                size_bytes=excluded.size_bytes,\n",
    "                created_ts=excluded.created_ts,\n",
    "                modified_ts=excluded.modified_ts,\n",
    "                sha1=excluded.sha1,\n",
    "                exists_flag=excluded.exists_flag,\n",
    "                read_error=excluded.read_error\n",
    "            \"\"\",\n",
    "            meta,\n",
    "        )\n",
    "        (file_id,) = con.execute(\n",
    "            \"SELECT file_id FROM files WHERE path = ?\",\n",
    "            (meta[\"path\"],),\n",
    "        ).fetchone()\n",
    "    return int(file_id)\n",
    "\n",
    "\n",
    "def replace_label(con: sqlite3.Connection, file_id: int, label: str) -> None:\n",
    "    with con:\n",
    "        con.execute(\"DELETE FROM labels WHERE file_id = ?\", (file_id,))\n",
    "        if label:\n",
    "            con.execute(\n",
    "                \"INSERT OR IGNORE INTO labels(file_id, business_category) VALUES (?, ?)\",\n",
    "                (file_id, label),\n",
    "            )\n",
    "\n",
    "\n",
    "def replace_content(con: sqlite3.Connection, file_id: int, text: str) -> None:\n",
    "    with con:\n",
    "        con.execute(\"DELETE FROM content WHERE file_id = ?\", (file_id,))\n",
    "        con.execute(\n",
    "            \"INSERT INTO content(file_id, content_text) VALUES (?, ?)\",\n",
    "            (file_id, text),\n",
    "        )\n",
    "\n",
    "\n",
    "def _resolve_column(df: pd.DataFrame, canonical: str, aliases: Sequence[str]) -> str:\n",
    "    if canonical in df.columns:\n",
    "        return canonical\n",
    "    for alias in aliases:\n",
    "        if alias in df.columns:\n",
    "            return alias\n",
    "    raise KeyError(\n",
    "        f\"Missing required column '{canonical}'. Available: {list(df.columns)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def ingest_from_excel(\n",
    "    excel_path: Path,\n",
    "    db_path: Path,\n",
    "    limit: Optional[int] = None,\n",
    ") -> Dict[str, int]:\n",
    "    ensure_db(db_path)\n",
    "    df = pd.read_excel(excel_path)\n",
    "    filepath_col = _resolve_column(df, FILEPATH_COLUMN, FILEPATH_ALIASES)\n",
    "    category_col = _resolve_column(df, CATEGORY_COLUMN, CATEGORY_ALIASES)\n",
    "    if limit is not None:\n",
    "        df = df.head(limit)\n",
    "    total_rows = len(df)\n",
    "    LOGGER.info(\"Starting ingestion of %s rows from %s\", total_rows, excel_path)\n",
    "    con = sqlite3.connect(str(db_path))\n",
    "    stats = {\"inserted\": 0, \"errors\": 0, \"empty_text\": 0, \"skipped_missing\": 0}\n",
    "    try:\n",
    "        rows = df[[filepath_col, category_col]].itertuples(index=False, name=None)\n",
    "        for idx, (filepath_value, category_value) in enumerate(rows, start=1):\n",
    "            if filepath_value is None or str(filepath_value).strip() == \"\":\n",
    "                stats[\"skipped_missing\"] += 1\n",
    "                continue\n",
    "            path_str = str(filepath_value)\n",
    "            LOGGER.info(\"Processing %s/%s: %s\", idx, total_rows, path_str)\n",
    "            meta = stat_path(path_str)\n",
    "            try:\n",
    "                file_id = upsert_file(con, meta)\n",
    "                replace_label(\n",
    "                    con,\n",
    "                    file_id,\n",
    "                    str(category_value) if category_value is not None else \"\",\n",
    "                )\n",
    "                text = \"\"\n",
    "                if meta[\"exists_flag\"]:\n",
    "                    text = extract_text(Path(path_str), meta[\"extension\"] or \"\")\n",
    "                if not text:\n",
    "                    stats[\"empty_text\"] += 1\n",
    "                replace_content(con, file_id, text)\n",
    "                stats[\"inserted\"] += 1\n",
    "            except Exception:\n",
    "                LOGGER.exception(\"Failed to ingest %s\", path_str)\n",
    "                stats[\"errors\"] += 1\n",
    "    finally:\n",
    "        con.close()\n",
    "    return stats\n",
    "\n",
    "\n",
    "def fetch_summary_rows(db_path: Path, limit: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "    con = sqlite3.connect(str(db_path))\n",
    "    con.row_factory = sqlite3.Row\n",
    "    query = \"\"\"\n",
    "        SELECT f.file_id,\n",
    "               f.file_name,\n",
    "               f.folder,\n",
    "               f.extension,\n",
    "               f.mime_type,\n",
    "               COALESCE(l.business_category, '') AS business_category,\n",
    "               f.exists_flag,\n",
    "               f.modified_ts,\n",
    "               f.size_bytes\n",
    "        FROM files f\n",
    "        LEFT JOIN labels l ON l.file_id = f.file_id\n",
    "        ORDER BY f.file_id\n",
    "    \"\"\"\n",
    "    params: Iterable[Any] = ()\n",
    "    if limit is not None:\n",
    "        query += \" LIMIT ?\"\n",
    "        params = (limit,)\n",
    "    rows = [dict(row) for row in con.execute(query, params).fetchall()]\n",
    "    con.close()\n",
    "    return rows\n",
    "\n",
    "\n",
    "def fetch_file_detail(db_path: Path, file_id: int) -> Optional[Dict[str, Any]]:\n",
    "    con = sqlite3.connect(str(db_path))\n",
    "    con.row_factory = sqlite3.Row\n",
    "    try:\n",
    "        file_row = con.execute(\n",
    "            \"SELECT * FROM files WHERE file_id = ?\",\n",
    "            (file_id,),\n",
    "        ).fetchone()\n",
    "        if not file_row:\n",
    "            return None\n",
    "        labels = [\n",
    "            r[0]\n",
    "            for r in con.execute(\n",
    "                \"SELECT business_category FROM labels WHERE file_id = ?\",\n",
    "                (file_id,),\n",
    "            ).fetchall()\n",
    "        ]\n",
    "        content_row = con.execute(\n",
    "            \"SELECT content_text FROM content WHERE file_id = ?\",\n",
    "            (file_id,),\n",
    "        ).fetchone()\n",
    "        return {\n",
    "            \"file\": dict(file_row),\n",
    "            \"labels\": labels,\n",
    "            \"content\": content_row[0] if content_row else \"\",\n",
    "        }\n",
    "    finally:\n",
    "        con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8b6ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_streamlit_runtime() -> bool:\n",
    "    if st is None:\n",
    "        return False\n",
    "    try:\n",
    "        from streamlit.runtime.scriptrunner import get_script_run_ctx\n",
    "    except Exception:\n",
    "        return False\n",
    "    return get_script_run_ctx() is not None\n",
    "\n",
    "\n",
    "def run_streamlit_app(db_path: Path, default_excel: Path) -> None:\n",
    "    if st is None:\n",
    "        raise RuntimeError(\"Streamlit is not available in this environment.\")\n",
    "\n",
    "    st.set_page_config(page_title=\"Document Metadata Browser\", layout=\"wide\")\n",
    "    st.title(\"Document Metadata Browser\")\n",
    "\n",
    "    if \"last_ingest_stats\" not in st.session_state:\n",
    "        st.session_state[\"last_ingest_stats\"] = None\n",
    "    if \"last_ingest_excel\" not in st.session_state:\n",
    "        st.session_state[\"last_ingest_excel\"] = str(default_excel)\n",
    "\n",
    "    with st.sidebar:\n",
    "        st.header(\"Ingestion\")\n",
    "        excel_input = st.text_input(\"Excel file\", st.session_state[\"last_ingest_excel\"])\n",
    "        limit_value = st.number_input(\n",
    "            \"Row limit (0 = all)\", min_value=0, step=1, value=0, format=\"%d\"\n",
    "        )\n",
    "        run_ingest = st.button(\"Run ingestion\")\n",
    "        if run_ingest:\n",
    "            excel_path = Path(excel_input)\n",
    "            if not excel_path.exists():\n",
    "                st.error(f\"Excel file not found: {excel_path}\")\n",
    "            else:\n",
    "                limit = limit_value if limit_value > 0 else None\n",
    "                with st.spinner(\"Ingesting files...\"):\n",
    "                    stats = ingest_from_excel(excel_path, db_path, limit=limit)\n",
    "                st.session_state[\"last_ingest_stats\"] = stats\n",
    "                st.session_state[\"last_ingest_excel\"] = excel_input\n",
    "                st.success(\n",
    "                    f\"Ingested {stats['inserted']} rows | \"\n",
    "                    f\"skipped_missing={stats['skipped_missing']} empty_text={stats['empty_text']} errors={stats['errors']}\"\n",
    "                )\n",
    "\n",
    "        st.markdown(\"---\")\n",
    "        st.subheader(\"Database\")\n",
    "        st.write(f\"Database path: `{db_path}`\")\n",
    "\n",
    "    stats = st.session_state.get(\"last_ingest_stats\")\n",
    "    if stats:\n",
    "        st.toast(\n",
    "            f\"Last ingest: inserted={stats['inserted']} | skipped_missing={stats['skipped_missing']} | \"\n",
    "            f\"empty_text={stats['empty_text']} | errors={stats['errors']}\",\n",
    "            icon=\"?\" if stats.get(\"errors\", 0) == 0 else \"??\",\n",
    "        )\n",
    "\n",
    "    rows = fetch_summary_rows(db_path)\n",
    "    if not rows:\n",
    "        st.info(\"No files in the database yet. Run an ingestion to populate the table.\")\n",
    "        return\n",
    "\n",
    "    df_summary = pd.DataFrame(rows)\n",
    "    st.subheader(\"Files\")\n",
    "    st.dataframe(df_summary, use_container_width=True, hide_index=True)\n",
    "\n",
    "    file_ids = df_summary[\"file_id\"].tolist()\n",
    "    if not file_ids:\n",
    "        st.warning(\"No files found.\")\n",
    "        return\n",
    "    selected_id = st.selectbox(\"Select file\", file_ids, index=0)\n",
    "    detail = fetch_file_detail(db_path, int(selected_id))\n",
    "    if not detail:\n",
    "        st.warning(\"No details found for the selected file.\")\n",
    "        return\n",
    "\n",
    "    file_meta = detail[\"file\"]\n",
    "    st.subheader(\"Metadata\")\n",
    "    st.json(file_meta)\n",
    "\n",
    "    labels = detail[\"labels\"] or [\"(unlabeled)\"]\n",
    "    st.markdown(\"**Business Capability:** \" + \", \".join(labels))\n",
    "\n",
    "    content_preview = detail[\"content\"][:2_000]\n",
    "    st.subheader(\"Content Preview\")\n",
    "    st.text_area(\n",
    "        \"Extracted text (first 2,000 characters)\",\n",
    "        value=content_preview or \"(no text extracted)\",\n",
    "        height=300,\n",
    "    )\n",
    "\n",
    "\n",
    "def launch_streamlit_process(\n",
    "    notebook_path: Path,\n",
    "    excel: Path,\n",
    "    db: Path,\n",
    "    limit: Optional[int],\n",
    "    skip_ingest: bool,\n",
    "    log_level: str,\n",
    ") -> None:\n",
    "    if st is None:\n",
    "        raise RuntimeError(\n",
    "            \"Streamlit is not installed. Install it with `pip install streamlit`.\"\n",
    "        )\n",
    "    streamlit_cli = shutil.which(\"streamlit\")\n",
    "    if not streamlit_cli:\n",
    "        raise RuntimeError(\"Streamlit CLI not found in PATH.\")\n",
    "    cmd = [\n",
    "        streamlit_cli,\n",
    "        \"run\",\n",
    "        str(notebook_path.resolve()),\n",
    "        \"--\",\n",
    "        \"--excel\",\n",
    "        str(excel.resolve()),\n",
    "        \"--db\",\n",
    "        str(db.resolve()),\n",
    "        \"--log-level\",\n",
    "        log_level,\n",
    "    ]\n",
    "    if limit is not None:\n",
    "        cmd.extend([\"--limit\", str(limit)])\n",
    "    if skip_ingest:\n",
    "        cmd.append(\"--skip-ingest\")\n",
    "    LOGGER.info(\"Starting Streamlit server... interrupt the kernel to stop it.\")\n",
    "    subprocess.run(cmd, check=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a1df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_logging(level: str) -> None:\n",
    "    root = logging.getLogger()\n",
    "    if not root.handlers:\n",
    "        logging.basicConfig(\n",
    "            level=getattr(logging, level.upper(), logging.INFO),\n",
    "            format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "        )\n",
    "    else:\n",
    "        root.setLevel(getattr(logging, level.upper(), logging.INFO))\n",
    "\n",
    "\n",
    "def get_runtime_args() -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser(add_help=False)\n",
    "    parser.add_argument(\"--excel\", type=Path, default=Path(\"trainingdata.xlsx\"))\n",
    "    parser.add_argument(\"--db\", type=Path, default=DB_PATH_DEFAULT)\n",
    "    parser.add_argument(\"--limit\", type=int, default=None)\n",
    "    parser.add_argument(\"--skip-ingest\", action=\"store_true\")\n",
    "    parser.add_argument(\"--no-ui\", action=\"store_true\")\n",
    "    parser.add_argument(\"--log-level\", default=\"INFO\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def run_metadata_app(\n",
    "    *,\n",
    "    excel: Path,\n",
    "    db: Path,\n",
    "    limit: Optional[int] = None,\n",
    "    skip_ingest: bool = False,\n",
    "    no_ui: bool = False,\n",
    "    log_level: str = \"INFO\",\n",
    "    notebook_path: Optional[Path] = None,\n",
    ") -> Optional[Dict[str, int]]:\n",
    "    configure_logging(log_level)\n",
    "    excel = Path(excel)\n",
    "    db = Path(db)\n",
    "    LOGGER.info(\"run_metadata_app start | excel=%s db=%s limit=%s skip_ingest=%s no_ui=%s\", excel, db, limit, skip_ingest, no_ui)\n",
    "    ensure_db(db)\n",
    "\n",
    "    stats: Optional[Dict[str, int]] = None\n",
    "    performed_ingestion = False\n",
    "\n",
    "    if not skip_ingest:\n",
    "        if excel.exists():\n",
    "            stats = ingest_from_excel(excel, db, limit=limit)\n",
    "            performed_ingestion = True\n",
    "            LOGGER.info(\n",
    "                \"Ingest complete | inserted=%s skipped_missing=%s empty_text=%s errors=%s\",\n",
    "                stats[\"inserted\"],\n",
    "                stats[\"skipped_missing\"],\n",
    "                stats[\"empty_text\"],\n",
    "                stats[\"errors\"],\n",
    "            )\n",
    "        else:\n",
    "            LOGGER.warning(\"Excel file not found: %s\", excel)\n",
    "    else:\n",
    "        LOGGER.info(\"Skipping ingestion step as requested.\")\n",
    "\n",
    "    if no_ui:\n",
    "        return stats\n",
    "\n",
    "    if is_streamlit_runtime():\n",
    "        run_streamlit_app(db, excel)\n",
    "        return stats\n",
    "\n",
    "    if notebook_path is None:\n",
    "        raise ValueError(\n",
    "            \"notebook_path must be provided when launching Streamlit from Jupyter.\"\n",
    "        )\n",
    "\n",
    "    downstream_skip_ingest = skip_ingest or performed_ingestion\n",
    "    launch_streamlit_process(\n",
    "        notebook_path=notebook_path,\n",
    "        excel=excel,\n",
    "        db=db,\n",
    "        limit=limit,\n",
    "        skip_ingest=downstream_skip_ingest,\n",
    "        log_level=log_level,\n",
    "    )\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2842be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook configuration\n",
    "EXCEL_PATH = Path(\"trainingdata.xlsx\")\n",
    "DB_PATH = DB_PATH_DEFAULT\n",
    "LIMIT_ROWS = None  # e.g. set to 50 to ingest only the first 50 rows\n",
    "SKIP_INGEST = False\n",
    "NO_UI = True  # Set to False to launch the Streamlit UI\n",
    "LOG_LEVEL = \"INFO\"\n",
    "NOTEBOOK_PATH = Path(\"reader_app.ipynb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a89851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ingestion and optionally launch the Streamlit UI\n",
    "stats = run_metadata_app(\n",
    "    excel=EXCEL_PATH,\n",
    "    db=DB_PATH,\n",
    "    limit=LIMIT_ROWS,\n",
    "    skip_ingest=SKIP_INGEST,\n",
    "    no_ui=NO_UI,\n",
    "    log_level=LOG_LEVEL,\n",
    "    notebook_path=NOTEBOOK_PATH,\n",
    ")\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0d8dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the files table (requires that the database exists)\n",
    "if DB_PATH.exists():\n",
    "    df_preview = pd.DataFrame(fetch_summary_rows(DB_PATH, limit=50))\n",
    "    display(df_preview.head(20))\n",
    "else:\n",
    "    print(f\"Database not found at {DB_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6fb47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this notebook is executed under Streamlit, bootstrap the UI automatically\n",
    "if is_streamlit_runtime():\n",
    "    runtime_args = get_runtime_args()\n",
    "    run_metadata_app(\n",
    "        excel=runtime_args.excel,\n",
    "        db=runtime_args.db,\n",
    "        limit=runtime_args.limit,\n",
    "        skip_ingest=runtime_args.skip_ingest,\n",
    "        no_ui=runtime_args.no_ui,\n",
    "        log_level=runtime_args.log_level,\n",
    "        notebook_path=None,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}